{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9d57097-59fe-4af6-aa4b-3cbf9bb24982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (24.1.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: torch in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (2.2.0+cu121)\n",
      "Requirement already satisfied: torchaudio in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (2.2.0+cu121)\n",
      "Requirement already satisfied: transformers in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (4.40.1)\n",
      "Requirement already satisfied: datasets in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (2.20.0)\n",
      "Requirement already satisfied: accelerate in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (0.33.0)\n",
      "Requirement already satisfied: evaluate in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (0.4.2)\n",
      "Requirement already satisfied: jiwer in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (3.0.4)\n",
      "Requirement already satisfied: huggingface_hub in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (0.22.2)\n",
      "Requirement already satisfied: yt-dlp in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (2024.7.25)\n",
      "Requirement already satisfied: tqdm in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (4.66.4)\n",
      "Requirement already satisfied: librosa in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (0.10.2.post1)\n",
      "Requirement already satisfied: soundfile in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (0.12.1)\n",
      "Requirement already satisfied: wandb in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (0.17.5)\n",
      "Requirement already satisfied: filelock in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch) (2024.2.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.1.105)\n",
      "Requirement already satisfied: numpy>=1.17 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from transformers) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from transformers) (2024.4.16)\n",
      "Requirement already satisfied: requests in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: xxhash in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: psutil in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from accelerate) (5.9.8)\n",
      "Requirement already satisfied: click<9.0.0,>=8.1.3 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from jiwer) (8.1.7)\n",
      "Requirement already satisfied: rapidfuzz<4,>=3 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from jiwer) (3.9.4)\n",
      "Requirement already satisfied: brotli in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from yt-dlp) (1.1.0)\n",
      "Requirement already satisfied: certifi in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from yt-dlp) (2024.2.2)\n",
      "Requirement already satisfied: mutagen in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from yt-dlp) (1.47.0)\n",
      "Requirement already satisfied: pycryptodomex in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from yt-dlp) (3.20.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.26.17 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from yt-dlp) (2.2.1)\n",
      "Requirement already satisfied: websockets>=12.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from yt-dlp) (12.0)\n",
      "Requirement already satisfied: audioread>=2.1.9 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from librosa) (3.0.1)\n",
      "Requirement already satisfied: scipy>=1.2.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from librosa) (1.14.0)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from librosa) (1.5.1)\n",
      "Requirement already satisfied: joblib>=0.14 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from librosa) (1.4.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from librosa) (5.1.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from librosa) (0.60.0)\n",
      "Requirement already satisfied: pooch>=1.1 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from librosa) (1.8.2)\n",
      "Requirement already satisfied: soxr>=0.3.2 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from librosa) (0.4.0)\n",
      "Requirement already satisfied: lazy-loader>=0.1 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from librosa) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from librosa) (1.0.8)\n",
      "Requirement already satisfied: cffi>=1.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from soundfile) (1.16.0)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from wandb) (3.1.43)\n",
      "Requirement already satisfied: platformdirs in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from wandb) (4.2.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<6,>=3.19.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from wandb) (5.27.2)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from wandb) (2.11.0)\n",
      "Requirement already satisfied: setproctitle in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from wandb) (1.3.3)\n",
      "Requirement already satisfied: setuptools in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from wandb) (68.2.2)\n",
      "Requirement already satisfied: pycparser in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from cffi>=1.0->soundfile) (2.22)\n",
      "Requirement already satisfied: six>=1.4.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from scikit-learn>=0.20.0->librosa) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install torch torchaudio transformers datasets datasets[audio] accelerate evaluate jiwer huggingface_hub yt-dlp tqdm librosa soundfile wandb\n",
    "!pip install -q -U bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42648db8-8670-4a2b-a1ac-180acbb38607",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5464b2ab76054d83bd4afee471dd855f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "819a8380-797d-4778-a49e-12f61cf30de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import load_from_disk\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import WhisperProcessor, AutoTokenizer, AutoModelForCausalLM, WhisperModel\n",
    "import yt_dlp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "from datasets import load_dataset\n",
    "from transformers import WhisperFeatureExtractor, WhisperForConditionalGeneration, BitsAndBytesConfig\n",
    "from datasets import Audio\n",
    "from torch.nn.utils.rnn import pad_sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a67ab8dc-70d8-4ae1-a9d2-fcfd59b7fe33",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a24e55fc-484b-4742-9891-257227953f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nice utility functions \n",
    "def text_2_ids_and_attention_mask(tokenizer, input_txt, truncate=False):\n",
    "    txt = input_txt\n",
    "    res = tokenizer(txt, return_tensors=\"pt\")\n",
    "\n",
    "    if truncate:\n",
    "        return res.input_ids[:, 1:], res.attention_mask[:, 1:]\n",
    "\n",
    "    return res.input_ids, res.attention_mask\n",
    "def prompt_template_fn(prompt=\"Describe the sound of the given file\"):\n",
    "    system_message = \"You are a helpful AI who follows instruction carefully\"\n",
    "\n",
    "    #mistral's prompt template\n",
    "    # prompt_prefix = f\"\"\"<|im_start|>system\n",
    "    # {system_message}<|im_end|>\n",
    "    # <|im_start|>user\n",
    "    # {prompt}\"\"\"\n",
    "\n",
    "    #Llama-3 prompt template\n",
    "    # https://llama.meta.com/docs/model-cards-and-prompt-formats/meta-llama-3/\n",
    "    prompt_prefix = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "    {system_message}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    {prompt}\n",
    "    \"\"\"\n",
    "    return prompt_prefix\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def end_template():\n",
    "    ###Mistral end token\n",
    "    # return \"\"\"\n",
    "    # <|im_end|>\n",
    "    # <|im_start|>assistant\n",
    "    # \"\"\"\n",
    "\n",
    "    #llama-3 end tokens\n",
    "    return \"\"\"\n",
    "        <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08ffdf11-0c0c-4e12-b94e-866a11d7ea3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 4337\n",
      "Validation dataset size: 482\n",
      "Test dataset size: 536\n"
     ]
    }
   ],
   "source": [
    "ds = load_dataset(\"mahendra0203/musiccaps_processed_full\" )\n",
    "main_dataset = ds['train'].train_test_split(test_size=0.1)\n",
    "\n",
    "train_val_split_dataset = main_dataset['train'].train_test_split(test_size=0.1)\n",
    "\n",
    "train_dataset = train_val_split_dataset['train']\n",
    "val_dataset = train_val_split_dataset['test']\n",
    "\n",
    "test_dataset = main_dataset['test']\n",
    "\n",
    "train_dataset = train_dataset.cast_column('audio', Audio(sampling_rate= 16000))\n",
    "val_dataset = val_dataset.cast_column('audio', Audio(sampling_rate= 16000))\n",
    "test_dataset = test_dataset.cast_column('audio', Audio(sampling_rate= 16000))\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0bdcaca1-8d57-4b2f-a548-dc0900a0f2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# train_dataset = load_from_disk(os.path.join('/home', \"dataset/train/train_musiccaps_audio_data\"))\n",
    "# val_dataset = load_from_disk(os.path.join('/home', \"dataset/val/val_musiccaps_audio_data\"))\n",
    "\n",
    "#START --This is the Rgiht code, testing with a small dataset noe\n",
    "# ds = load_dataset(\"mahendra0203/musiccaps_processed_full\" )\n",
    "# main_dataset = ds['train'].train_test_split(test_size=0.1)\n",
    "#END--\n",
    "\n",
    "# ds = load_dataset(\"mahendra0203/musiccaps_processed\" )\n",
    "# small_dataset = ds['train'].train_test_split(test_size=0.05)\n",
    "# main_dataset = small_dataset['test'].train_test_split(test_size = 0.1)\n",
    "\n",
    "# # train_dataset = load_dataset(\"mahendra0203/musiccaps_processed\")\n",
    "# # main_dataset = load_dataset(\"mahendra0203/musiccaps_processed\")\n",
    "# train_dataset = main_dataset['train']\n",
    "# val_dataset = main_dataset['test']\n",
    "\n",
    "# # #cast audio to sample rate of 16000\n",
    "# train_dataset = train_dataset.cast_column('audio', Audio(sampling_rate= 16000))\n",
    "# val_dataset = val_dataset.cast_column('audio', Audio(sampling_rate= 16000))\n",
    "\n",
    "# print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "# print(f\"Validation dataset size: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e28173a-75e6-4c06-845a-24a39a8d24f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'audio': {'path': 'yt_id_qrzNABqN420_30_40.wav',\n",
       "  'array': array([-0.31793213, -0.25042725, -0.29745483, ..., -0.08570862,\n",
       "          0.09069824, -0.08700562]),\n",
       "  'sampling_rate': 16000},\n",
       " 'caption': \"The low quality recording features a rock song that consists of a passionate female vocal singing over punchy kick and snare hits, shimmering hi hats, wide electric guitar melody and groovy bass guitar. It sounds energetic and addictive in the first half of the loop, while in the second part it's mellow and softer.\",\n",
       " 'youtube_id': 'qrzNABqN420',\n",
       " 'start_time': 30,\n",
       " 'end_time': 40,\n",
       " 'aspect_list': \"['low quality', 'rock', 'passionate female vocal', 'wide electric guitar melody', 'groovy bass', 'shimmering hi hats', 'punchy kick', 'punchy kick', 'mellow', 'energetic', 'addictive', 'soft']\"}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd7194ee-4fbf-4478-8c90-484f6e928a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "whisper_model_name = \"openai/whisper-large-v2\"\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(whisper_model_name)\n",
    "whisper_processor = WhisperProcessor.from_pretrained(whisper_model_name)\n",
    "#Download the llm tokenizer.LLM will be downloaded in the model\n",
    "# model_name = \"facebook/opt-1.3b\"\n",
    "model_name= \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# llm = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3613506-c3d9-448e-b2d4-efcaa2bd50a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bnb_config = BitsAndBytesConfig(\n",
    "#             load_in_4bit=True,\n",
    "#             bnb_4bit_use_double_quant=True,\n",
    "#             bnb_4bit_quant_type=\"nf4\",\n",
    "#             bnb_4bit_compute_dtype=torch.bfloat16\n",
    "#         )\n",
    "# llm = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d01ac04a-31ce-4ea6-98fa-9b9ae419b724",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(batch):\n",
    "    # load and resample audio data from 48 to 16kHz\n",
    "\n",
    "    audio = batch[\"audio\"]\n",
    "    # compute log-Mel input features from input audio array \n",
    "    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
    "    # encode target text to label ids \n",
    "    # batch[\"caption\"] = batch[\"caption\"]\n",
    "    token_result = tokenizer(batch[\"caption\"])\n",
    "    batch[\"caption_ids\"] = token_result.input_ids\n",
    "    batch[\"caption_attention_mask\"] = token_result.attention_mask\n",
    "\n",
    "    # Encode prompt\n",
    "    prompt_tokens = tokenizer(prompt_template_fn())\n",
    "    batch[\"prompt_ids\"] = prompt_tokens.input_ids\n",
    "    batch[\"prompt_attention_mask\"] = prompt_tokens.attention_mask\n",
    "    \n",
    "    # Encode end prompt\n",
    "    end_prompt_tokens = tokenizer(end_template())\n",
    "    batch[\"end_prompt_ids\"] = end_prompt_tokens.input_ids\n",
    "    batch[\"end_prompt_attention_mask\"] = end_prompt_tokens.attention_mask\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0af3752e-d392-42c4-9cef-67f339fcb8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# new_train_dataset = train_dataset.map(prepare_dataset, remove_columns=train_dataset.column_names, num_proc=64)\n",
    "# new_val_dataset = val_dataset.map(prepare_dataset, remove_columns=val_dataset.column_names, num_proc=64)\n",
    "# new_test_dataset = test_dataset.map(prepare_dataset, remove_columns=test_dataset.column_names, num_proc=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3d34b13-62bf-41df-8182-40aea4752ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_train_dataset.save_to_disk('/home/mapped_dataset/train/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "50f899dd-51af-4eb0-bd4f-8292fb09c86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_val_dataset.save_to_disk('/home/mapped_dataset/val/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fad148a6-5836-4287-a499-eed426d1bfcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_test_dataset.save_to_disk('/home/mapped_dataset/test/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d4cc5e6d-2dd9-46bf-8723-a3e52562326e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_train_dataset=None\n",
    "# new_val_dataset=None\n",
    "# new_test_dataset=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2efebd74-4ec9-49e5-a1a4-531d437f6eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_dataset = load_from_disk('/home/mapped_dataset/train/')\n",
    "new_val_dataset = load_from_disk('/home/mapped_dataset/val/')\n",
    "new_test_dataset = load_from_disk('/home/mapped_dataset/test/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee48f21b-6442-445b-bcaa-0497b10e14b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b66e27-e98b-4ecc-847f-743c8fc96fc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7779d6-5bbb-4f6a-b6f7-107cb0393db5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "81bf644d-67a9-47fb-9733-4286426cd2fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input features shape: torch.Size([8, 80, 3000])\n",
      "Caption IDs shape: torch.Size([8, 90])\n",
      "Caption attention mask shape: torch.Size([8, 90])\n",
      "Prompt IDs shape: torch.Size([8, 31])\n",
      "Prompt attention mask shape: torch.Size([8, 31])\n",
      "End prompt IDs shape: torch.Size([8, 9])\n",
      "End prompt attention mask shape: torch.Size([8, 9])\n",
      "DataLoader creation completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33161/3557587238.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_features = torch.stack([torch.tensor(x) for x in input_features])\n"
     ]
    }
   ],
   "source": [
    "# Set batch size\n",
    "def collate_fn(batch):\n",
    "    # Separate different components of the batch\n",
    "    input_features = [whisper_processor.feature_extractor.pad({\"input_features\": item['input_features']}, return_tensors=\"pt\").input_features for item in batch]\n",
    "    # formatted_features = processor.feature_extractor.pad({\"input_features\": input_features}, return_tensors=\"pt\")\n",
    "\n",
    "    caption_ids = [item['caption_ids'] for item in batch]\n",
    "    caption_attention_mask = [item['caption_attention_mask'] for item in batch]\n",
    "    prompt_ids = [item['prompt_ids'] for item in batch]\n",
    "    prompt_attention_mask = [item['prompt_attention_mask'] for item in batch]\n",
    "    end_prompt_ids = [item['end_prompt_ids'] for item in batch]\n",
    "    end_prompt_attention_mask = [item['end_prompt_attention_mask'] for item in batch]\n",
    "\n",
    "    # # Pad sequences\n",
    "    #128009 is end of message token -> <|eot_id|> for llama3\n",
    "    caption_ids = pad_sequence([torch.tensor(x) for x in caption_ids], batch_first=True, padding_value=128009)\n",
    "    caption_attention_mask = pad_sequence([torch.tensor(x) for x in caption_attention_mask], batch_first=True, padding_value=128009)\n",
    "    prompt_ids = pad_sequence([torch.tensor(x) for x in prompt_ids], batch_first=True, padding_value=128009)\n",
    "    prompt_attention_mask = pad_sequence([torch.tensor(x) for x in prompt_attention_mask], batch_first=True, padding_value=128009)\n",
    "    end_prompt_ids = pad_sequence([torch.tensor(x) for x in end_prompt_ids], batch_first=True, padding_value=128009)\n",
    "    end_prompt_attention_mask = pad_sequence([torch.tensor(x) for x in end_prompt_attention_mask], batch_first=True, padding_value=128009)\n",
    "\n",
    "    # caption_ids = pad_sequence([torch.tensor(x) for x in caption_ids], batch_first=True, padding_value=0)\n",
    "    # caption_attention_mask = pad_sequence([torch.tensor(x) for x in caption_attention_mask], batch_first=True, padding_value=0)\n",
    "    # prompt_ids = pad_sequence([torch.tensor(x) for x in prompt_ids], batch_first=True, padding_value=0)\n",
    "    # prompt_attention_mask = pad_sequence([torch.tensor(x) for x in prompt_attention_mask], batch_first=True, padding_value=0)\n",
    "    # end_prompt_ids = pad_sequence([torch.tensor(x) for x in end_prompt_ids], batch_first=True, padding_value=0)\n",
    "    # end_prompt_attention_mask = pad_sequence([torch.tensor(x) for x in end_prompt_attention_mask], batch_first=True, padding_value=0)\n",
    "    \n",
    "    # # Stack input features (assuming they're already of the same size)\n",
    "    input_features = torch.stack([torch.tensor(x) for x in input_features])\n",
    "\n",
    "    return {\n",
    "        'input_features': input_features,\n",
    "        'caption_ids': caption_ids,\n",
    "        'caption_attention_mask': caption_attention_mask,\n",
    "        'prompt_ids': prompt_ids,\n",
    "        'prompt_attention_mask': prompt_attention_mask,\n",
    "        'end_prompt_ids': end_prompt_ids,\n",
    "        'end_prompt_attention_mask': end_prompt_attention_mask\n",
    "    }\n",
    "batch_size = 8  # Adjust this value based on your GPU memory and model size\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataloader = DataLoader(new_train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "val_dataloader = DataLoader(new_val_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "test_dataloader = DataLoader(new_test_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# Verify the batches\n",
    "for batch in train_dataloader:\n",
    "    print(\"Input features shape:\", batch['input_features'].shape)\n",
    "    print(\"Caption IDs shape:\", batch['caption_ids'].shape)\n",
    "    print(\"Caption attention mask shape:\", batch['caption_attention_mask'].shape)\n",
    "    print(\"Prompt IDs shape:\", batch['prompt_ids'].shape)\n",
    "    print(\"Prompt attention mask shape:\", batch['prompt_attention_mask'].shape)\n",
    "    print(\"End prompt IDs shape:\", batch['end_prompt_ids'].shape)\n",
    "    print(\"End prompt attention mask shape:\", batch['end_prompt_attention_mask'].shape)\n",
    "    # print('\\n'.join(tokenizer.batch_decode(batch['caption_ids'][-10: ])))\n",
    "\n",
    "    break\n",
    "\n",
    "print(\"DataLoader creation completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f65ad32-b0c5-497a-bf58-4adc80557eb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bc71551e-16c9-4e7e-b068-357f639efa72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tmp_batch = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "de68796b-a862-4e31-8f02-5ccce59d6a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader_iter = iter(train_dataloader)\n",
    "# batch = next(dataloader_iter)\n",
    "# batch.keys()\n",
    "\n",
    "# len(batch['input_features'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ecfc8eaf-7ce6-4544-86c4-114ea650524f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "# tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fcf93892-46a9-4ee0-b65b-806bd2262c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.encode('<|eot_id|>',  padding=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808fc1d4-caa4-4fa4-9f07-e3082a9f90ef",
   "metadata": {},
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d2fe4bed-b616-4169-adc1-1f97e097b26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TunableWhisperProjection(nn.Module):\n",
    "    def __init__(self, input_embedding_size=1280, output_embedding_size=2048):\n",
    "        super().__init__()\n",
    "\n",
    "        self.pool = nn.AdaptiveAvgPool1d(250)\n",
    "        self.proj = nn.Linear(input_embedding_size, output_embedding_size, bias=False)\n",
    "        self.ln1 = nn.LayerNorm(input_embedding_size)\n",
    "\n",
    "    def forward(self, whisper_output):\n",
    "        # Assuming whisper_output is of shape (batch_size, seq_len, input_embedding_size)\n",
    "        # Transpose for adaptive pooling\n",
    "        pooled = self.pool(whisper_output.transpose(-2, -1))\n",
    "        \n",
    "        # Transpose back and apply layer norm\n",
    "        normalized = self.ln1(pooled.transpose(-2, -1))\n",
    "        \n",
    "        # Project to output embedding size\n",
    "        projected = self.proj(normalized)\n",
    "        \n",
    "        return projected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b4316458-c544-4545-bfa2-1db384913f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiModalLLM(nn.Module):\n",
    "    def __init__(self, whisper_model_name=\"openai/whisper-large-v2\", llm_model_name=\"facebook/opt-1.3b\"):\n",
    "        super().__init__()\n",
    "        # Audio encoder (Whisper)\n",
    "        bnb_whsiper_config = BitsAndBytesConfig(\n",
    "            load_in_8bit=True,\n",
    "        )\n",
    "        \n",
    "        self.audio_encoder = WhisperModel.from_pretrained(whisper_model_name).get_encoder()\n",
    "        # Freeze Whisper parameters\n",
    "        for param in self.audio_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        # Language Model\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16\n",
    "        )\n",
    "        self.llm = AutoModelForCausalLM.from_pretrained(llm_model_name, quantization_config=bnb_config)\n",
    "        for param in self.llm.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.projection = TunableWhisperProjection(input_embedding_size=1280, output_embedding_size=4096)\n",
    "\n",
    "    def forward(self, input_features, prompt_ids, prompt_attention_mask, \n",
    "                end_prompt_ids, end_prompt_attention_mask, \n",
    "                caption_ids, caption_attention_mask):\n",
    "        # print the shape of all these above\n",
    "        # print(f\"input_features.shape: {input_features.shape}\")\n",
    "        # print(f\"prompt_ids.shape: {prompt_ids.shape}\")\n",
    "        # print(f\"prompt_attention_mask.shape: {prompt_attention_mask.shape}\")\n",
    "        # print(f\"end_prompt_ids.shape: {end_prompt_ids.shape}\")\n",
    "        # print(f\"end_prompt_attention_mask.shape: {end_prompt_attention_mask.shape}\")\n",
    "        # print(f\"caption_ids.shape: {caption_ids.shape}\")\n",
    "        # print(f\"caption_attention_mask.shape: {caption_attention_mask.shape}\")\n",
    "        \n",
    "        # Process audio with Whisper encoder\n",
    "        audio_outputs = self.audio_encoder.forward(input_features).last_hidden_state\n",
    "        # print(f\"audio_outputs.shape: {audio_outputs.shape}\")\n",
    "        projected_audio = self.projection(audio_outputs)\n",
    "        # print(f\"projected_audio.shape: {projected_audio.shape}\")\n",
    "        # caption_ids = caption_ids.to(device)\n",
    "        # prompt_ids = prompt_ids.to(device)\n",
    "        # end_prompt_ids = end_prompt_ids.to(device)\n",
    "\n",
    "        if(self.llm.model.name_or_path == \"meta-llama/Meta-Llama-3-8B-Instruct\"):\n",
    "            cap_embeds = self.llm.model.embed_tokens(caption_ids)\n",
    "            prompt_embeds = self.llm.model.embed_tokens(prompt_ids)\n",
    "            end_prompt_embeds = self.llm.model.embed_tokens(end_prompt_ids)\n",
    "        else:\n",
    "            cap_embeds = self.llm.model.get_decoder().embed_tokens(caption_ids)\n",
    "            prompt_embeds = self.llm.model.get_decoder().embed_tokens(prompt_ids)\n",
    "            end_prompt_embeds = self.llm.model.get_decoder().embed_tokens(end_prompt_ids)\n",
    "\n",
    "        # print(f\"cap_embeds.shape: {cap_embeds.shape}\")\n",
    "        # print(f\"prompt_embeds.shape: {prompt_embeds.shape}\")\n",
    "        # print(f\"end_prompt_embeds.shape: {end_prompt_embeds.shape}\")\n",
    "        \n",
    "        bs, audio_seq = projected_audio.shape[:2]\n",
    "\n",
    "        # print(f\"batch_size: {bs}\")\n",
    "        # print(f\"audio_seq: {audio_seq}\")\n",
    "        \n",
    "        inputs_embeds = torch.concat(\n",
    "            (\n",
    "                prompt_embeds,\n",
    "                projected_audio.to(cap_embeds.dtype),\n",
    "                end_prompt_embeds,\n",
    "                cap_embeds,\n",
    "            ),\n",
    "            dim=1,\n",
    "        )\n",
    "        # print(f\"inputs_embeds.shape: {inputs_embeds.shape}\")\n",
    "        \n",
    "        attention_mask = torch.concat(\n",
    "            (\n",
    "                prompt_attention_mask,\n",
    "                torch.ones(bs, audio_seq).to(caption_ids.device),\n",
    "                end_prompt_attention_mask,\n",
    "                caption_attention_mask,\n",
    "            ),\n",
    "            dim=1,\n",
    "        )\n",
    "        # print(f\"attention_mask.shape: {attention_mask.shape}\")\n",
    "        \n",
    "        outputs = self.llm(\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        return outputs, audio_seq\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "1cff1b4d-2eaf-471f-88c5-842b4f73c29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c59a261b-155a-4197-a1cd-da8ee2cd7f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login(key=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a4c4d7e6-890d-439d-8fe9-9f182903a094",
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_model_to_gpu(model):\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "        for param in model.parameters():\n",
    "            param.data = param.data.cuda()\n",
    "            if param.grad is not None:\n",
    "                param.grad.data = param.grad.data.cuda()\n",
    "    return model\n",
    "    \n",
    "def move_to_device(batch, device):\n",
    "    return {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2334e9e9-4d05-43e5-8657-514317397c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_results_to_wandb(results):\n",
    "    table = wandb.Table(columns=[\"Ground Truth\", \"Generated\"])\n",
    "    for result in results:\n",
    "        table.add_data(result[\"Ground Truth\"], result[\"Generated\"])\n",
    "    wandb.log({\"Caption Comparison\": table})\n",
    "    \n",
    "def run_inference(model, dataloader, tokenizer, device, num_samples=10):\n",
    "    model.eval()\n",
    "    results = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(tqdm(dataloader, desc=\"Running inference\")):\n",
    "            if i >= num_samples:\n",
    "                break\n",
    "            \n",
    "            batch = move_to_device(batch, device)\n",
    "            \n",
    "            outputs, audio_len = model(**batch)\n",
    "            prompt_ids = batch['prompt_ids']\n",
    "            end_prompt_ids = batch['end_prompt_ids']\n",
    "            caption_ids = batch['caption_ids']\n",
    "            \n",
    "            prompt_ids_seq = prompt_ids.shape[1]\n",
    "            end_prompt_ids_seq = end_prompt_ids.shape[1]\n",
    "            audio_seq = audio_len\n",
    "            logits_start = prompt_ids_seq + audio_seq + end_prompt_ids_seq\n",
    "            \n",
    "            logits = outputs.logits\n",
    "            op_logits = logits[:, logits_start:-1, :].contiguous()\n",
    "            caption_labels = caption_ids[:, 1:].contiguous()\n",
    "            \n",
    "            sampled = torch.multinomial(op_logits[:, -1, :].softmax(dim=-1), caption_labels.shape[1])\n",
    "            \n",
    "            ground_truth = tokenizer.batch_decode(batch['caption_ids'], skip_special_tokens=True)\n",
    "            generated = tokenizer.batch_decode(sampled,skip_special_tokens=True)\n",
    "            \n",
    "            for gt, gen in zip(ground_truth, generated):\n",
    "                results.append({\"Ground Truth\": gt, \"Generated\": gen})\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4df178ee-bbc5-4f5f-8836-7bdf96e5cb61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a7c2bcbc5c846eb906e280b4cbf7286",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/wandb/run-20240726_092621-f4bdfbug</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jutsu-labs/MultiLLM%20-%20whisper-large-v2%20LLAMA%203-8b%20Instruct/runs/f4bdfbug' target=\"_blank\">epoch_10_lr_1.5e-3 -- padding fixes</a></strong> to <a href='https://wandb.ai/jutsu-labs/MultiLLM%20-%20whisper-large-v2%20LLAMA%203-8b%20Instruct' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jutsu-labs/MultiLLM%20-%20whisper-large-v2%20LLAMA%203-8b%20Instruct' target=\"_blank\">https://wandb.ai/jutsu-labs/MultiLLM%20-%20whisper-large-v2%20LLAMA%203-8b%20Instruct</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jutsu-labs/MultiLLM%20-%20whisper-large-v2%20LLAMA%203-8b%20Instruct/runs/f4bdfbug' target=\"_blank\">https://wandb.ai/jutsu-labs/MultiLLM%20-%20whisper-large-v2%20LLAMA%203-8b%20Instruct/runs/f4bdfbug</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/543 [00:00<?, ?it/s]/tmp/ipykernel_33161/3557587238.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_features = torch.stack([torch.tensor(x) for x in input_features])\n",
      "Training: 100%|██████████| 543/543 [34:53<00:00,  3.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 4.9006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:   0%|          | 0/61 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Validation Loss: 0.0659\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running inference:  12%|█▏        | 8/67 [00:24<02:59,  3.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 543/543 [33:50<00:00,  3.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 3.9957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:   0%|          | 0/61 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Validation Loss: 0.0589\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running inference:  12%|█▏        | 8/67 [00:23<02:56,  3.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 543/543 [33:48<00:00,  3.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 3.6720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:   0%|          | 0/61 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Validation Loss: 0.0692\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running inference:  12%|█▏        | 8/67 [00:24<02:57,  3.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 543/543 [33:55<00:00,  3.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 3.5319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:   0%|          | 0/61 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Validation Loss: 0.0499\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running inference:  12%|█▏        | 8/67 [00:24<02:59,  3.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 543/543 [33:57<00:00,  3.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 3.4590\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:   0%|          | 0/61 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Validation Loss: 0.0622\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running inference:  12%|█▏        | 8/67 [00:24<03:00,  3.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 543/543 [34:05<00:00,  3.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 3.3974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:   0%|          | 0/61 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Validation Loss: 0.0479\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running inference:  12%|█▏        | 8/67 [00:24<02:57,  3.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 543/543 [33:59<00:00,  3.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 3.3338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:   0%|          | 0/61 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Validation Loss: 0.0600\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running inference:  12%|█▏        | 8/67 [00:24<02:57,  3.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 543/543 [33:59<00:00,  3.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 3.3015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:   0%|          | 0/61 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Validation Loss: 0.0635\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running inference:  12%|█▏        | 8/67 [00:23<02:55,  2.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 543/543 [33:58<00:00,  3.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 3.2555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:   0%|          | 0/61 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Validation Loss: 0.0745\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running inference:  12%|█▏        | 8/67 [00:23<02:54,  2.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 543/543 [33:42<00:00,  3.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 3.2535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:   0%|          | 0/61 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Validation Loss: 0.0538\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running inference:  12%|█▏        | 8/67 [00:23<02:55,  2.98s/it]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='1.296 MB of 1.296 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_loss</td><td>█▄▃▂▂▂▁▁▁▁</td></tr><tr><td>val_loss</td><td>▆▄▇▂▅▁▄▅█▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>train_loss</td><td>3.25346</td></tr><tr><td>val_loss</td><td>0.05377</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">epoch_10_lr_1.5e-3 -- padding fixes</strong> at: <a href='https://wandb.ai/jutsu-labs/MultiLLM%20-%20whisper-large-v2%20LLAMA%203-8b%20Instruct/runs/f4bdfbug' target=\"_blank\">https://wandb.ai/jutsu-labs/MultiLLM%20-%20whisper-large-v2%20LLAMA%203-8b%20Instruct/runs/f4bdfbug</a><br/> View project at: <a href='https://wandb.ai/jutsu-labs/MultiLLM%20-%20whisper-large-v2%20LLAMA%203-8b%20Instruct' target=\"_blank\">https://wandb.ai/jutsu-labs/MultiLLM%20-%20whisper-large-v2%20LLAMA%203-8b%20Instruct</a><br/>Synced 6 W&B file(s), 10 media file(s), 10 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240726_092621-f4bdfbug/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "import wandb\n",
    "\n",
    "def calculate_loss(outputs, batch, audio_seq):\n",
    "    logits = outputs.logits\n",
    "    prompt_ids = batch['prompt_ids']\n",
    "    end_prompt_ids = batch['end_prompt_ids']\n",
    "    caption_ids = batch['caption_ids']\n",
    "    # proj_op = outputs.projected_audio  # Assuming your model returns this\n",
    "    # print(prompt_ids.shape)\n",
    "    # print(end_prompt_ids.shape)\n",
    "    # print(caption_ids.shape)\n",
    "    # print(audio_seq)\n",
    "    \n",
    "    prompt_ids_seq = prompt_ids.shape[1]\n",
    "    end_prompt_ids_seq = end_prompt_ids.shape[1]\n",
    "    audio_seq = audio_seq\n",
    "    logits_start = prompt_ids_seq + audio_seq + end_prompt_ids_seq\n",
    "    # print(f\"Logits shape: {logits.shape}\")\n",
    "    # print(f\"Logits start: {logits_start}\")\n",
    "    # print(f\"Caption IDs shape: {caption_ids.shape}\")\n",
    "\n",
    "    # Ensure logits_start is not out of bounds\n",
    "    # if logits_start >= logits.shape[1]:\n",
    "    #     raise ValueError(f\"logits_start ({logits_start}) is out of bounds for logits shape {logits.shape}\")\n",
    "\n",
    "    op_logits = logits[:, logits_start:-1, :].contiguous()\n",
    "    caption_labels = caption_ids[:, 1:].contiguous()\n",
    "    \n",
    "    # print(f\"op_logits shape: {op_logits.shape}\")\n",
    "    # print(f\"caption_labels shape: {caption_labels.shape}\")\n",
    "    \n",
    "    if op_logits.shape[1] != caption_labels.shape[1]:\n",
    "        raise ValueError(f\"Shape mismatch: op_logits {op_logits.shape}, caption_labels {caption_labels.shape}\")\n",
    "\n",
    "    \n",
    "    loss = F.cross_entropy(\n",
    "        op_logits.view(-1, op_logits.shape[-1]), caption_labels.view(-1)\n",
    "    )\n",
    "    return loss\n",
    "    \n",
    "\n",
    "    \n",
    "def train(model, train_dataloader, val_dataloader, num_epochs, learning_rate, device, wandb_proj_name=\"MultiModal LLM\", wandb_run_name=\"Training Run\"):\n",
    "    wandb.init(project=wandb_proj_name, name=wandb_run_name)\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    wandb.config.update({\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"epochs\": num_epochs,\n",
    "        \"batch_size\": train_dataloader.batch_size,\n",
    "        \"model_name\": type(model).__name__,\n",
    "        # Add any other hyperparameters you want to track\n",
    "    })\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        # Training\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        for batch in tqdm(train_dataloader, desc=\"Training\"):\n",
    "            # batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            batch = move_to_device(batch, device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs, audio_seq = model(**batch)\n",
    "            loss = calculate_loss(outputs, batch, audio_seq)\n",
    "            # loss = outputs.loss\n",
    "            # print(f\"loss:{loss.item()}\")\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_train_loss += loss.item()\n",
    "            # break;\n",
    "            \n",
    "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "        print(f\"Average Training Loss: {avg_train_loss:.4f}\")\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_dataloader, desc=\"Validating\"):\n",
    "                batch = move_to_device(batch, device)\n",
    "                # batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                outputs, audio_seq = model(**batch)\n",
    "                # loss = outputs.loss\n",
    "                # outputs = model(**batch)\n",
    "                loss = calculate_loss(outputs, batch, audio_seq)\n",
    "                total_val_loss += loss.item()\n",
    "                \n",
    "        avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "        print(f\"Average Validation Loss: {avg_val_loss:.4f}\")\n",
    "        print()\n",
    "        \n",
    "        # Log metrics to wandb\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"train_loss\": avg_train_loss,\n",
    "            \"val_loss\": avg_val_loss\n",
    "        })\n",
    "        # Run inference on a batch and log results\n",
    "        inference_results = run_inference(model, test_dataloader, tokenizer, device, num_samples=8)\n",
    "        log_results_to_wandb(inference_results)\n",
    "        # Optionally, log model parameters\n",
    "        wandb.watch(model)\n",
    "\n",
    "    torch.save(model.state_dict(), \"final_model.pth\")\n",
    "    wandb.save(\"final_model.pth\")\n",
    "\n",
    "    # Finish the wandb run\n",
    "    wandb.finish()\n",
    "\n",
    "\n",
    "# Setup\n",
    "# model = MultimodalAudioTextModel()  # Make sure this is defined\n",
    "model = MultiModalLLM(whisper_model_name, model_name)\n",
    "model = move_model_to_gpu(model)\n",
    "num_epochs = 10\n",
    "learning_rate = 1.5e-3\n",
    "\n",
    "# Assuming train_dataloader and val_dataloader are already created\n",
    "train(model, train_dataloader, val_dataloader, num_epochs, learning_rate, device,wandb_proj_name=\"MultiLLM - whisper-large-v2 LLAMA 3-8b Instruct\",  wandb_run_name=\"epoch_10_lr_1.5e-3 -- padding fixes\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "32e1f4ce-cb5d-4fe4-a944-4b48bb61f0ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13921"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model = model.cpu()\n",
    "    \n",
    "# # Delete the model\n",
    "# del model\n",
    "\n",
    "# torch.cuda.empty_cache()\n",
    "    \n",
    "# # Force garbage collection\n",
    "# import gc\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e7c0102f-1d08-4745-b2df-cf3e288bc5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def log_results_to_wandb(results):\n",
    "#     table = wandb.Table(columns=[\"Ground Truth\", \"Generated\"])\n",
    "#     for result in results:\n",
    "#         table.add_data(result[\"Ground Truth\"], result[\"Generated\"])\n",
    "#     wandb.log({\"Caption Comparison\": table})\n",
    "    \n",
    "# def run_inference(model, dataloader, tokenizer, device, num_samples=10):\n",
    "#     model.eval()\n",
    "#     results = []\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         for i, batch in enumerate(tqdm(dataloader, desc=\"Running inference\")):\n",
    "#             if i >= num_samples:\n",
    "#                 break\n",
    "            \n",
    "#             batch = move_to_device(batch, device)\n",
    "            \n",
    "#             outputs, audio_len = model(**batch)\n",
    "#             prompt_ids = batch['prompt_ids']\n",
    "#             end_prompt_ids = batch['end_prompt_ids']\n",
    "#             caption_ids = batch['caption_ids']\n",
    "            \n",
    "#             prompt_ids_seq = prompt_ids.shape[1]\n",
    "#             end_prompt_ids_seq = end_prompt_ids.shape[1]\n",
    "#             audio_seq = audio_len\n",
    "#             logits_start = prompt_ids_seq + audio_seq + end_prompt_ids_seq\n",
    "            \n",
    "#             logits = outputs.logits\n",
    "#             op_logits = logits[:, logits_start:-1, :].contiguous()\n",
    "#             caption_labels = caption_ids[:, 1:].contiguous()\n",
    "            \n",
    "#             sampled = torch.multinomial(op_logits[:, -1, :].softmax(dim=-1), caption_labels.shape[1])\n",
    "            \n",
    "#             ground_truth = tokenizer.batch_decode(batch['caption_ids'])\n",
    "#             generated = tokenizer.batch_decode(sampled)\n",
    "            \n",
    "#             for gt, gen in zip(ground_truth, generated):\n",
    "#                 results.append({\"Ground Truth\": gt, \"Generated\": gen})\n",
    "    \n",
    "#     return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "44aa1e74-cadf-4adb-b515-0e1a9645f8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_inference(model, val_dataloader, tokenizer, device, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "59a6bdf2-ce0e-4b40-b333-ff34265441c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #run inference\n",
    "\n",
    "# def run_inference(model, dataloader, tokenizer, sample, device):\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         new_batch = next(iter(dataloader))\n",
    "#         new_batch = move_to_device(new_batch, device)\n",
    "#         # tmp_batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in tmp_batch.items()}\n",
    "            \n",
    "#         outputs, audio_len = model(**new_batch)\n",
    "#         prompt_ids = new_batch['prompt_ids']\n",
    "#         end_prompt_ids = new_batch['end_prompt_ids']\n",
    "#         caption_ids = new_batch['caption_ids']\n",
    "                    \n",
    "#         prompt_ids_seq = prompt_ids.shape[1]\n",
    "#         end_prompt_ids_seq = end_prompt_ids.shape[1]\n",
    "#         audio_seq = audio_len\n",
    "#         logits_start = prompt_ids_seq + audio_seq + end_prompt_ids_seq\n",
    "            \n",
    "#         logits = outputs.logits\n",
    "#         op_logits = logits[:, logits_start:-1, :].contiguous()\n",
    "#         caption_labels = caption_ids[:, 1:].contiguous()\n",
    "            \n",
    "#         sampled = torch.multinomial(op_logits[:, -1, :].softmax(dim=-1), caption_labels.shape[1])\n",
    "            \n",
    "#         print(f\"ground truth caption: {tokenizer.batch_decode(new_batch['caption_ids'])}\")\n",
    "#         print(f\"model outout caption: {tokenizer.batch_decode(sampled)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6eca86b0-f528-4aac-990c-398213245fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tmp_batch = next(iter(val_dataloader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "42c5523d-6aa5-4cb0-a812-983053e1dd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tmp_batch = move_to_device(batch, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7dd9debd-a0e5-4560-8ae7-5a9128d034b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ops = model(**tmp_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "39db5de3-1721-48f3-9e0f-f38dae05a2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = run_inference(model, val_dataloader, tokenizer, device, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c583ac62-a6c4-47b7-abc2-3ec23a8c60f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "33faf406-e520-48d6-bef1-84ff740bca62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(0, len(tmp_batch['caption_ids'])):\n",
    "#     print(len(tmp_batch['caption_ids'][i]))\n",
    "# len(tmp_batch['caption_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8758563d-2d14-40ec-bc1e-ca5825263fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.encode('<|eot_id|>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "711ca8d4-024a-4f1d-ba41-0d0ff0e8e031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.batch_decode(tmp_batch['caption_ids'], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d7d878-f056-4ab1-8f98-872ea420aee2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c847b2d-6a2c-4f0e-8d3d-b2c74f3d348e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "18935c2e-8cc7-4c2b-ac5b-b35dbe78e255",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from typing import Dict, List, Tuple\n",
    "from transformers.trainer_utils import EvalLoopOutput\n",
    "import numpy as np\n",
    "\n",
    "class MultiModalDataset(Dataset):\n",
    "    def __init__(self, dataloader):\n",
    "        self.dataloader = dataloader\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataloader.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataloader.dataset[idx]\n",
    "\n",
    "class MultiModalTrainer(Trainer):\n",
    "    def __init__(self, test_dataloader, tokenizer, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.test_dataloader = test_dataloader\n",
    "        self.test_step = 0\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def training_step(self, model, inputs):\n",
    "        \"\"\"\n",
    "        Perform a training step and run test loop every 500 steps.\n",
    "        \"\"\"\n",
    "        # Perform regular training step\n",
    "        loss = super().training_step(model, inputs)\n",
    "\n",
    "        # Increment test step counter\n",
    "        self.test_step += 1\n",
    "\n",
    "        # Run test loop every 500 steps\n",
    "        # for test purposes logging at every step\n",
    "        if self.test_step % 500 == 0:\n",
    "            self.run_test_loop()\n",
    "\n",
    "        return loss\n",
    "        \n",
    "    def run_test_loop(self):\n",
    "        inference_results = run_inference(self.model, \n",
    "                                          self.test_dataloader, \n",
    "                                          self.tokenizer, \n",
    "                                          device, \n",
    "                                          num_samples=2)\n",
    "        log_results_to_wandb(inference_results)\n",
    "        \n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        outputs, audio_seq = model(**inputs)\n",
    "        loss = self.calculate_loss(outputs, inputs, audio_seq)\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "    def calculate_loss(self, outputs, batch, audio_seq):\n",
    "        logits = outputs.logits\n",
    "        prompt_ids = batch['prompt_ids']\n",
    "        end_prompt_ids = batch['end_prompt_ids']\n",
    "        caption_ids = batch['caption_ids']\n",
    "        \n",
    "        prompt_ids_seq = prompt_ids.shape[1]\n",
    "        end_prompt_ids_seq = end_prompt_ids.shape[1]\n",
    "        logits_start = prompt_ids_seq + audio_seq + end_prompt_ids_seq\n",
    "\n",
    "        op_logits = logits[:, logits_start:-1, :].contiguous()\n",
    "        caption_labels = caption_ids[:, 1:].contiguous()\n",
    "        \n",
    "        if op_logits.shape[1] != caption_labels.shape[1]:\n",
    "            raise ValueError(f\"Shape mismatch: op_logits {op_logits.shape}, caption_labels {caption_labels.shape}\")\n",
    "\n",
    "        loss = torch.nn.functional.cross_entropy(\n",
    "            op_logits.view(-1, op_logits.shape[-1]), caption_labels.view(-1)\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "    def evaluation_loop(\n",
    "        self,\n",
    "        dataloader,\n",
    "        description: str,\n",
    "        prediction_loss_only: bool = None,\n",
    "        ignore_keys: List[str] = None,\n",
    "        metric_key_prefix: str = \"eval\",\n",
    "    ) -> Tuple[Dict[str, float], List[torch.Tensor]]:\n",
    "        # Custom evaluation loop\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        total_samples = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "\n",
    "        for batch in dataloader:\n",
    "            with torch.no_grad():\n",
    "                outputs, audio_seq = self.model(**batch)\n",
    "                loss = self.calculate_loss(outputs, batch, audio_seq)\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                logits = outputs.logits\n",
    "                preds = torch.argmax(logits, dim=-1)\n",
    "                labels = batch['caption_ids']\n",
    "                \n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "\n",
    "        # Calculate additional metrics (e.g., accuracy)\n",
    "        # accuracy = np.mean([p == l for p, l in zip(all_preds, all_labels)])\n",
    "\n",
    "        metrics = {\n",
    "            f\"eval\": avg_loss\n",
    "        }\n",
    "        return EvalLoopOutput(predictions=all_preds, label_ids=all_labels, metrics=metrics, num_samples=total_samples)\n",
    "\n",
    "        # return EvalLoopOutput( metrics=metrics, num_samples=len(dataloader))\n",
    "\n",
    "        # return metrics, all_preds\n",
    "\n",
    "# wandb.init(project=\"MultiModal LLM\", name=\"HF Trainer Run\")\n",
    "# Prepare datasets\n",
    "train_dataset = MultiModalDataset(train_dataloader)\n",
    "eval_dataset = MultiModalDataset(val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b32eec3d-01e4-44bd-b927-49c2805ab29b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e479cda17be847d6b0c9059aa2bd2455",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = MultiModalLLM(whisper_model_name, model_name)\n",
    "model = move_model_to_gpu(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "80882ee7-324e-4058-80c6-827523252dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No valid checkpoint found in output directory (./results_version_3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[92], line 33\u001b[0m\n\u001b[1;32m     22\u001b[0m trainer \u001b[38;5;241m=\u001b[39m MultiModalTrainer(\n\u001b[1;32m     23\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     24\u001b[0m         args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     29\u001b[0m         tokenizer\u001b[38;5;241m=\u001b[39mtokenizer\n\u001b[1;32m     30\u001b[0m     )\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Save the final model\u001b[39;00m\n\u001b[1;32m     36\u001b[0m trainer\u001b[38;5;241m.\u001b[39msave_model(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinal_model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/transformers/trainer.py:1827\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1825\u001b[0m     resume_from_checkpoint \u001b[38;5;241m=\u001b[39m get_last_checkpoint(args\u001b[38;5;241m.\u001b[39moutput_dir)\n\u001b[1;32m   1826\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m resume_from_checkpoint \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1827\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo valid checkpoint found in output directory (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;241m.\u001b[39moutput_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1829\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resume_from_checkpoint \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1830\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_sagemaker_mp_enabled() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_deepspeed_enabled \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_fsdp_enabled:\n",
      "\u001b[0;31mValueError\u001b[0m: No valid checkpoint found in output directory (./results_version_3)"
     ]
    }
   ],
   "source": [
    "# Define training arguments\n",
    "# wandb.init(project=\"MultiLLM- 27-7-24\", name=\"12 epochs-lr 1.5e3\", settings=wandb.Settings(start_method=\"fork\"))\n",
    "training_args = TrainingArguments(\n",
    "        output_dir=\"./results_version_3\",\n",
    "        num_train_epochs=12,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        warmup_steps=500,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir=\"./logs\",\n",
    "        logging_steps=100,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=500,\n",
    "        save_steps=5000,\n",
    "        max_steps=10,\n",
    "        load_best_model_at_end=True,\n",
    "        learning_rate=1.5e-3,\n",
    "        do_eval=False,\n",
    "        report_to=\"wandb\",\n",
    "    )\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = MultiModalTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=collate_fn,\n",
    "        train_dataset=new_train_dataset,\n",
    "        eval_dataset=new_val_dataset,\n",
    "        test_dataloader=test_dataloader,\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "\n",
    "# Train the model\n",
    "trainer.train(resume_from_checkpoint=True)\n",
    "\n",
    "# Save the final model\n",
    "trainer.save_model(\"final_model\")\n",
    "trainer.save_state()\n",
    "\n",
    "torch.save(model.state_dict(), \"final_model.pth\")\n",
    "wandb.save(\"final_model.pth\")\n",
    "# Finish the wandb run\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f6be1799-a55c-43d7-bb1f-ff5025d61e8c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/wandb/run-20240726_213821-00ita3cc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jutsu-labs/MultiLLM-%2027-7-24/runs/00ita3cc' target=\"_blank\">exalted-fire-3</a></strong> to <a href='https://wandb.ai/jutsu-labs/MultiLLM-%2027-7-24' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jutsu-labs/MultiLLM-%2027-7-24' target=\"_blank\">https://wandb.ai/jutsu-labs/MultiLLM-%2027-7-24</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jutsu-labs/MultiLLM-%2027-7-24/runs/00ita3cc' target=\"_blank\">https://wandb.ai/jutsu-labs/MultiLLM-%2027-7-24/runs/00ita3cc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/jutsu-labs/MultiLLM-%2027-7-24/runs/00ita3cc?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7faea9f3f0a0>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# wandb.init(project=\"MultiLLM- 27-7-24\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4344d9-38dd-4c51-9731-76ee7fda173e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
