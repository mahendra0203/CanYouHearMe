{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d57097-59fe-4af6-aa4b-3cbf9bb24982",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install torch torchaudio transformers datasets datasets[audio] accelerate evaluate jiwer huggingface_hub yt-dlp tqdm librosa soundfile wandb\n",
    "!pip install -q -U bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42648db8-8670-4a2b-a1ac-180acbb38607",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "362e84a4a87346a18e10f08c596f8e64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "819a8380-797d-4778-a49e-12f61cf30de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import load_from_disk\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import WhisperProcessor, AutoTokenizer, AutoModelForCausalLM, WhisperModel\n",
    "import yt_dlp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "from datasets import load_dataset\n",
    "from transformers import WhisperFeatureExtractor, WhisperForConditionalGeneration, BitsAndBytesConfig\n",
    "from datasets import Audio\n",
    "from torch.nn.utils.rnn import pad_sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a67ab8dc-70d8-4ae1-a9d2-fcfd59b7fe33",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a24e55fc-484b-4742-9891-257227953f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nice utility functions \n",
    "def text_2_ids_and_attention_mask(tokenizer, input_txt, truncate=False):\n",
    "    txt = input_txt\n",
    "    res = tokenizer(txt, return_tensors=\"pt\")\n",
    "\n",
    "    if truncate:\n",
    "        return res.input_ids[:, 1:], res.attention_mask[:, 1:]\n",
    "\n",
    "    return res.input_ids, res.attention_mask\n",
    "def prompt_template_fn(prompt=\"Describe the sound of the given file\"):\n",
    "    system_message = \"You are a helpful AI who follows instruction carefully\"\n",
    "\n",
    "    #mistral's prompt template\n",
    "    # prompt_prefix = f\"\"\"<|im_start|>system\n",
    "    # {system_message}<|im_end|>\n",
    "    # <|im_start|>user\n",
    "    # {prompt}\"\"\"\n",
    "\n",
    "    #Llama-3 prompt template\n",
    "    # https://llama.meta.com/docs/model-cards-and-prompt-formats/meta-llama-3/\n",
    "    prompt_prefix = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "    {system_message}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    {prompt}\n",
    "    \"\"\"\n",
    "    return prompt_prefix\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def end_template():\n",
    "    ###Mistral end token\n",
    "    # return \"\"\"\n",
    "    # <|im_end|>\n",
    "    # <|im_start|>assistant\n",
    "    # \"\"\"\n",
    "\n",
    "    #llama-3 end tokens\n",
    "    return \"\"\"\n",
    "        <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08ffdf11-0c0c-4e12-b94e-866a11d7ea3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d9d41d0baba4372b9c8ccddc8b900a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9068600e2614b208f4c99ea99e9724e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/483M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30a1af9fc29f4e428bd5969ae40d205c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/481M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a3d7642a09f4e4393f2c537173320cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/476M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e84e795bc6f4ae3b162703b24941b59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/474M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29cecedc3f7040fa930c521c84376c26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/472M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1ce52c538ba41848e76128c1262144e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/476M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3622765b6934417adf84b6bae53dc67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/469M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "948132e1611b4e3b81acb0d5aa423640",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/5355 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 4337\n",
      "Validation dataset size: 482\n",
      "Test dataset size: 536\n"
     ]
    }
   ],
   "source": [
    "ds = load_dataset(\"mahendra0203/musiccaps_processed_full\" )\n",
    "main_dataset = ds['train'].train_test_split(test_size=0.1)\n",
    "\n",
    "train_val_split_dataset = main_dataset['train'].train_test_split(test_size=0.1)\n",
    "\n",
    "train_dataset = train_val_split_dataset['train']\n",
    "val_dataset = train_val_split_dataset['test']\n",
    "\n",
    "test_dataset = main_dataset['test']\n",
    "\n",
    "train_dataset = train_dataset.cast_column('audio', Audio(sampling_rate= 16000))\n",
    "val_dataset = val_dataset.cast_column('audio', Audio(sampling_rate= 16000))\n",
    "test_dataset = test_dataset.cast_column('audio', Audio(sampling_rate= 16000))\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0bdcaca1-8d57-4b2f-a548-dc0900a0f2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# train_dataset = load_from_disk(os.path.join('/home', \"dataset/train/train_musiccaps_audio_data\"))\n",
    "# val_dataset = load_from_disk(os.path.join('/home', \"dataset/val/val_musiccaps_audio_data\"))\n",
    "\n",
    "#START --This is the Rgiht code, testing with a small dataset noe\n",
    "# ds = load_dataset(\"mahendra0203/musiccaps_processed_full\" )\n",
    "# main_dataset = ds['train'].train_test_split(test_size=0.1)\n",
    "#END--\n",
    "\n",
    "# ds = load_dataset(\"mahendra0203/musiccaps_processed\" )\n",
    "# small_dataset = ds['train'].train_test_split(test_size=0.05)\n",
    "# main_dataset = small_dataset['test'].train_test_split(test_size = 0.1)\n",
    "\n",
    "# # train_dataset = load_dataset(\"mahendra0203/musiccaps_processed\")\n",
    "# # main_dataset = load_dataset(\"mahendra0203/musiccaps_processed\")\n",
    "# train_dataset = main_dataset['train']\n",
    "# val_dataset = main_dataset['test']\n",
    "\n",
    "# # #cast audio to sample rate of 16000\n",
    "# train_dataset = train_dataset.cast_column('audio', Audio(sampling_rate= 16000))\n",
    "# val_dataset = val_dataset.cast_column('audio', Audio(sampling_rate= 16000))\n",
    "\n",
    "# print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "# print(f\"Validation dataset size: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e28173a-75e6-4c06-845a-24a39a8d24f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'audio': {'path': 'yt_id_vy2gUzgmBzQ_180_190.wav',\n",
       "  'array': array([-0.05502319,  0.56330872,  0.58569336, ...,  0.32862854,\n",
       "         -0.21427917, -0.25509644]),\n",
       "  'sampling_rate': 16000},\n",
       " 'caption': 'This goth metal song features a male voice singing the main melody. The voice is deep. Toward the end, there are other male voices singing vocables. This is accompanied by heavy percussion using a five-beat common time pattern played on floor toms with a shuffle note each note creating a triplet feel. The distortion guitars play heavy descending chord riffs using palm muting technique. The bass guitar plays the root notes of the chords. At the background, there is an high pitched sound played on a synth that gives this song an eerie feel. This song can be played at the end credits of a horror movie.',\n",
       " 'youtube_id': 'vy2gUzgmBzQ',\n",
       " 'start_time': 180,\n",
       " 'end_time': 190,\n",
       " 'aspect_list': \"['heavy metal', 'goth metal', 'male voice', 'amateur recording', 'distortion guitars', 'heavy percussion', 'descending chords', 'synth backing', 'eerie']\"}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd7194ee-4fbf-4478-8c90-484f6e928a9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b06e4a0dba3d4c00bdf5f542f3195973",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/185k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8f910963e854ce2ad9c1d1b5341e564",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/283k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f47b5d3f0aa54448871ea13fa37fc3e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/836k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5442c55003094d1ca4daac3e5f076258",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.48M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c02d37a9b0d4c238d535056588eea6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/494k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be5fce283c3143e3a0bedeee36c468ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "normalizer.json:   0%|          | 0.00/52.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "786d80bd3c164f188c28503ea45d545e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/34.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb488e3f64c04d37b362432544ccb0fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.19k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f7d88536eef4bd5bad0880a0e73e614",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/51.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "827518e11fe847c0a29e3269007bb291",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16432ce8347243d4a0e6dd07c8a42a85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/73.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "whisper_model_name = \"openai/whisper-large-v2\"\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(whisper_model_name)\n",
    "whisper_processor = WhisperProcessor.from_pretrained(whisper_model_name)\n",
    "#Download the llm tokenizer.LLM will be downloaded in the model\n",
    "# model_name = \"facebook/opt-1.3b\"\n",
    "model_name= \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# llm = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3613506-c3d9-448e-b2d4-efcaa2bd50a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bnb_config = BitsAndBytesConfig(\n",
    "#             load_in_4bit=True,\n",
    "#             bnb_4bit_use_double_quant=True,\n",
    "#             bnb_4bit_quant_type=\"nf4\",\n",
    "#             bnb_4bit_compute_dtype=torch.bfloat16\n",
    "#         )\n",
    "# llm = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d01ac04a-31ce-4ea6-98fa-9b9ae419b724",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(batch):\n",
    "    # load and resample audio data from 48 to 16kHz\n",
    "\n",
    "    audio = batch[\"audio\"]\n",
    "    # compute log-Mel input features from input audio array \n",
    "    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
    "    # encode target text to label ids \n",
    "    # batch[\"caption\"] = batch[\"caption\"]\n",
    "    token_result = tokenizer(batch[\"caption\"])\n",
    "    batch[\"caption_ids\"] = token_result.input_ids\n",
    "    batch[\"caption_attention_mask\"] = token_result.attention_mask\n",
    "\n",
    "    # Encode prompt\n",
    "    prompt_tokens = tokenizer(prompt_template_fn())\n",
    "    batch[\"prompt_ids\"] = prompt_tokens.input_ids\n",
    "    batch[\"prompt_attention_mask\"] = prompt_tokens.attention_mask\n",
    "    \n",
    "    # Encode end prompt\n",
    "    end_prompt_tokens = tokenizer(end_template())\n",
    "    batch[\"end_prompt_ids\"] = end_prompt_tokens.input_ids\n",
    "    batch[\"end_prompt_attention_mask\"] = end_prompt_tokens.attention_mask\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0af3752e-d392-42c4-9cef-67f339fcb8e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ee6dd65717b4cef9cbd9bb811743779",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=64):   0%|          | 0/4337 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6f471d9792e408081e557220d240861",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=64):   0%|          | 0/482 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9621aec5b6a94e92820471ce6d1e769a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=64):   0%|          | 0/536 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "new_train_dataset = train_dataset.map(prepare_dataset, remove_columns=train_dataset.column_names, num_proc=64)\n",
    "new_val_dataset = val_dataset.map(prepare_dataset, remove_columns=val_dataset.column_names, num_proc=64)\n",
    "new_test_dataset = test_dataset.map(prepare_dataset, remove_columns=test_dataset.column_names, num_proc=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3d34b13-62bf-41df-8182-40aea4752ffe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "853c2871dc4940dfae15538f6d584316",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/9 shards):   0%|          | 0/4337 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "new_train_dataset.save_to_disk('/home/mapped_dataset/train/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "50f899dd-51af-4eb0-bd4f-8292fb09c86b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2f1c01eb1c7477eac694ad9f2a23697",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/482 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "new_val_dataset.save_to_disk('/home/mapped_dataset/val/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fad148a6-5836-4287-a499-eed426d1bfcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ceaa6e646b144ce5ac17929f9b00bff6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/2 shards):   0%|          | 0/536 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "new_test_dataset.save_to_disk('/home/mapped_dataset/test/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d4cc5e6d-2dd9-46bf-8723-a3e52562326e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_train_dataset=None\n",
    "# new_val_dataset=None\n",
    "# new_test_dataset=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2efebd74-4ec9-49e5-a1a4-531d437f6eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_train_dataset = load_from_disk('/home/mapped_dataset/train/')\n",
    "# new_val_dataset = load_from_disk('/home/mapped_dataset/val/')\n",
    "# new_test_dataset = load_from_disk('/home/mapped_dataset/test/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee48f21b-6442-445b-bcaa-0497b10e14b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b66e27-e98b-4ecc-847f-743c8fc96fc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7779d6-5bbb-4f6a-b6f7-107cb0393db5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "81bf644d-67a9-47fb-9733-4286426cd2fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input features shape: torch.Size([8, 80, 3000])\n",
      "Caption IDs shape: torch.Size([8, 76])\n",
      "Caption attention mask shape: torch.Size([8, 76])\n",
      "Prompt IDs shape: torch.Size([8, 31])\n",
      "Prompt attention mask shape: torch.Size([8, 31])\n",
      "End prompt IDs shape: torch.Size([8, 9])\n",
      "End prompt attention mask shape: torch.Size([8, 9])\n",
      "DataLoader creation completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_146/3557587238.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_features = torch.stack([torch.tensor(x) for x in input_features])\n"
     ]
    }
   ],
   "source": [
    "# Set batch size\n",
    "def collate_fn(batch):\n",
    "    # Separate different components of the batch\n",
    "    input_features = [whisper_processor.feature_extractor.pad({\"input_features\": item['input_features']}, return_tensors=\"pt\").input_features for item in batch]\n",
    "    # formatted_features = processor.feature_extractor.pad({\"input_features\": input_features}, return_tensors=\"pt\")\n",
    "\n",
    "    caption_ids = [item['caption_ids'] for item in batch]\n",
    "    caption_attention_mask = [item['caption_attention_mask'] for item in batch]\n",
    "    prompt_ids = [item['prompt_ids'] for item in batch]\n",
    "    prompt_attention_mask = [item['prompt_attention_mask'] for item in batch]\n",
    "    end_prompt_ids = [item['end_prompt_ids'] for item in batch]\n",
    "    end_prompt_attention_mask = [item['end_prompt_attention_mask'] for item in batch]\n",
    "\n",
    "    # # Pad sequences\n",
    "    #128009 is end of message token -> <|eot_id|> for llama3\n",
    "    caption_ids = pad_sequence([torch.tensor(x) for x in caption_ids], batch_first=True, padding_value=128009)\n",
    "    caption_attention_mask = pad_sequence([torch.tensor(x) for x in caption_attention_mask], batch_first=True, padding_value=128009)\n",
    "    prompt_ids = pad_sequence([torch.tensor(x) for x in prompt_ids], batch_first=True, padding_value=128009)\n",
    "    prompt_attention_mask = pad_sequence([torch.tensor(x) for x in prompt_attention_mask], batch_first=True, padding_value=128009)\n",
    "    end_prompt_ids = pad_sequence([torch.tensor(x) for x in end_prompt_ids], batch_first=True, padding_value=128009)\n",
    "    end_prompt_attention_mask = pad_sequence([torch.tensor(x) for x in end_prompt_attention_mask], batch_first=True, padding_value=128009)\n",
    "\n",
    "    # caption_ids = pad_sequence([torch.tensor(x) for x in caption_ids], batch_first=True, padding_value=0)\n",
    "    # caption_attention_mask = pad_sequence([torch.tensor(x) for x in caption_attention_mask], batch_first=True, padding_value=0)\n",
    "    # prompt_ids = pad_sequence([torch.tensor(x) for x in prompt_ids], batch_first=True, padding_value=0)\n",
    "    # prompt_attention_mask = pad_sequence([torch.tensor(x) for x in prompt_attention_mask], batch_first=True, padding_value=0)\n",
    "    # end_prompt_ids = pad_sequence([torch.tensor(x) for x in end_prompt_ids], batch_first=True, padding_value=0)\n",
    "    # end_prompt_attention_mask = pad_sequence([torch.tensor(x) for x in end_prompt_attention_mask], batch_first=True, padding_value=0)\n",
    "    \n",
    "    # # Stack input features (assuming they're already of the same size)\n",
    "    input_features = torch.stack([torch.tensor(x) for x in input_features])\n",
    "\n",
    "    return {\n",
    "        'input_features': input_features,\n",
    "        'caption_ids': caption_ids,\n",
    "        'caption_attention_mask': caption_attention_mask,\n",
    "        'prompt_ids': prompt_ids,\n",
    "        'prompt_attention_mask': prompt_attention_mask,\n",
    "        'end_prompt_ids': end_prompt_ids,\n",
    "        'end_prompt_attention_mask': end_prompt_attention_mask\n",
    "    }\n",
    "batch_size = 8  # Adjust this value based on your GPU memory and model size\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataloader = DataLoader(new_train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "val_dataloader = DataLoader(new_val_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "test_dataloader = DataLoader(new_test_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# Verify the batches\n",
    "for batch in train_dataloader:\n",
    "    print(\"Input features shape:\", batch['input_features'].shape)\n",
    "    print(\"Caption IDs shape:\", batch['caption_ids'].shape)\n",
    "    print(\"Caption attention mask shape:\", batch['caption_attention_mask'].shape)\n",
    "    print(\"Prompt IDs shape:\", batch['prompt_ids'].shape)\n",
    "    print(\"Prompt attention mask shape:\", batch['prompt_attention_mask'].shape)\n",
    "    print(\"End prompt IDs shape:\", batch['end_prompt_ids'].shape)\n",
    "    print(\"End prompt attention mask shape:\", batch['end_prompt_attention_mask'].shape)\n",
    "    # print('\\n'.join(tokenizer.batch_decode(batch['caption_ids'][-10: ])))\n",
    "\n",
    "    break\n",
    "\n",
    "print(\"DataLoader creation completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f65ad32-b0c5-497a-bf58-4adc80557eb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bc71551e-16c9-4e7e-b068-357f639efa72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tmp_batch = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "de68796b-a862-4e31-8f02-5ccce59d6a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader_iter = iter(train_dataloader)\n",
    "# batch = next(dataloader_iter)\n",
    "# batch.keys()\n",
    "\n",
    "# len(batch['input_features'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ecfc8eaf-7ce6-4544-86c4-114ea650524f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "# tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fcf93892-46a9-4ee0-b65b-806bd2262c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.encode('<|eot_id|>',  padding=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808fc1d4-caa4-4fa4-9f07-e3082a9f90ef",
   "metadata": {},
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d2fe4bed-b616-4169-adc1-1f97e097b26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TunableWhisperProjection(nn.Module):\n",
    "    def __init__(self, input_embedding_size=1280, output_embedding_size=2048):\n",
    "        super().__init__()\n",
    "\n",
    "        self.pool = nn.AdaptiveAvgPool1d(250)\n",
    "        self.proj = nn.Linear(input_embedding_size, output_embedding_size, bias=False)\n",
    "        self.ln1 = nn.LayerNorm(input_embedding_size)\n",
    "\n",
    "    def forward(self, whisper_output):\n",
    "        # Assuming whisper_output is of shape (batch_size, seq_len, input_embedding_size)\n",
    "        # Transpose for adaptive pooling\n",
    "        pooled = self.pool(whisper_output.transpose(-2, -1))\n",
    "        \n",
    "        # Transpose back and apply layer norm\n",
    "        normalized = self.ln1(pooled.transpose(-2, -1))\n",
    "        \n",
    "        # Project to output embedding size\n",
    "        projected = self.proj(normalized)\n",
    "        \n",
    "        return projected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b4316458-c544-4545-bfa2-1db384913f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiModalLLM(nn.Module):\n",
    "    def __init__(self, whisper_model_name=\"openai/whisper-large-v2\", llm_model_name=\"facebook/opt-1.3b\"):\n",
    "        super().__init__()\n",
    "        # Audio encoder (Whisper)\n",
    "        bnb_whsiper_config = BitsAndBytesConfig(\n",
    "            load_in_8bit=True,\n",
    "        )\n",
    "        \n",
    "        self.audio_encoder = WhisperModel.from_pretrained(whisper_model_name).get_encoder()\n",
    "        # Freeze Whisper parameters\n",
    "        for param in self.audio_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        # Language Model\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16\n",
    "        )\n",
    "        self.llm = AutoModelForCausalLM.from_pretrained(llm_model_name, quantization_config=bnb_config)\n",
    "        for param in self.llm.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.projection = TunableWhisperProjection(input_embedding_size=1280, output_embedding_size=4096)\n",
    "\n",
    "    def forward(self, input_features, prompt_ids, prompt_attention_mask, \n",
    "                end_prompt_ids, end_prompt_attention_mask, \n",
    "                caption_ids, caption_attention_mask):\n",
    "        # print the shape of all these above\n",
    "        # print(f\"input_features.shape: {input_features.shape}\")\n",
    "        # print(f\"prompt_ids.shape: {prompt_ids.shape}\")\n",
    "        # print(f\"prompt_attention_mask.shape: {prompt_attention_mask.shape}\")\n",
    "        # print(f\"end_prompt_ids.shape: {end_prompt_ids.shape}\")\n",
    "        # print(f\"end_prompt_attention_mask.shape: {end_prompt_attention_mask.shape}\")\n",
    "        # print(f\"caption_ids.shape: {caption_ids.shape}\")\n",
    "        # print(f\"caption_attention_mask.shape: {caption_attention_mask.shape}\")\n",
    "        \n",
    "        # Process audio with Whisper encoder\n",
    "        audio_outputs = self.audio_encoder.forward(input_features).last_hidden_state\n",
    "        # print(f\"audio_outputs.shape: {audio_outputs.shape}\")\n",
    "        projected_audio = self.projection(audio_outputs)\n",
    "        # print(f\"projected_audio.shape: {projected_audio.shape}\")\n",
    "        # caption_ids = caption_ids.to(device)\n",
    "        # prompt_ids = prompt_ids.to(device)\n",
    "        # end_prompt_ids = end_prompt_ids.to(device)\n",
    "\n",
    "        if(self.llm.model.name_or_path == \"meta-llama/Meta-Llama-3-8B-Instruct\"):\n",
    "            cap_embeds = self.llm.model.embed_tokens(caption_ids)\n",
    "            prompt_embeds = self.llm.model.embed_tokens(prompt_ids)\n",
    "            end_prompt_embeds = self.llm.model.embed_tokens(end_prompt_ids)\n",
    "        else:\n",
    "            cap_embeds = self.llm.model.get_decoder().embed_tokens(caption_ids)\n",
    "            prompt_embeds = self.llm.model.get_decoder().embed_tokens(prompt_ids)\n",
    "            end_prompt_embeds = self.llm.model.get_decoder().embed_tokens(end_prompt_ids)\n",
    "\n",
    "        # print(f\"cap_embeds.shape: {cap_embeds.shape}\")\n",
    "        # print(f\"prompt_embeds.shape: {prompt_embeds.shape}\")\n",
    "        # print(f\"end_prompt_embeds.shape: {end_prompt_embeds.shape}\")\n",
    "        \n",
    "        bs, audio_seq = projected_audio.shape[:2]\n",
    "\n",
    "        # print(f\"batch_size: {bs}\")\n",
    "        # print(f\"audio_seq: {audio_seq}\")\n",
    "        \n",
    "        inputs_embeds = torch.concat(\n",
    "            (\n",
    "                prompt_embeds,\n",
    "                projected_audio.to(cap_embeds.dtype),\n",
    "                end_prompt_embeds,\n",
    "                cap_embeds,\n",
    "            ),\n",
    "            dim=1,\n",
    "        )\n",
    "        # print(f\"inputs_embeds.shape: {inputs_embeds.shape}\")\n",
    "        \n",
    "        attention_mask = torch.concat(\n",
    "            (\n",
    "                prompt_attention_mask,\n",
    "                torch.ones(bs, audio_seq).to(caption_ids.device),\n",
    "                end_prompt_attention_mask,\n",
    "                caption_attention_mask,\n",
    "            ),\n",
    "            dim=1,\n",
    "        )\n",
    "        # print(f\"attention_mask.shape: {attention_mask.shape}\")\n",
    "        \n",
    "        outputs = self.llm(\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        return outputs, audio_seq\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1cff1b4d-2eaf-471f-88c5-842b4f73c29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c59a261b-155a-4197-a1cd-da8ee2cd7f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login(key=\"0bce4133f74d94dae8633922fa2e8760e280f639\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a4c4d7e6-890d-439d-8fe9-9f182903a094",
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_model_to_gpu(model):\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "        for param in model.parameters():\n",
    "            param.data = param.data.cuda()\n",
    "            if param.grad is not None:\n",
    "                param.grad.data = param.grad.data.cuda()\n",
    "    return model\n",
    "    \n",
    "def move_to_device(batch, device):\n",
    "    return {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2334e9e9-4d05-43e5-8657-514317397c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_results_to_wandb(results):\n",
    "    table = wandb.Table(columns=[\"Ground Truth\", \"Generated\"])\n",
    "    for result in results:\n",
    "        table.add_data(result[\"Ground Truth\"], result[\"Generated\"])\n",
    "    wandb.log({\"Caption Comparison\": table})\n",
    "    \n",
    "def run_inference(model, dataloader, tokenizer, device, num_samples=10):\n",
    "    model.eval()\n",
    "    results = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(tqdm(dataloader, desc=\"Running inference\")):\n",
    "            if i >= num_samples:\n",
    "                break\n",
    "            \n",
    "            batch = move_to_device(batch, device)\n",
    "            \n",
    "            outputs, audio_len = model(**batch)\n",
    "            prompt_ids = batch['prompt_ids']\n",
    "            end_prompt_ids = batch['end_prompt_ids']\n",
    "            caption_ids = batch['caption_ids']\n",
    "            \n",
    "            prompt_ids_seq = prompt_ids.shape[1]\n",
    "            end_prompt_ids_seq = end_prompt_ids.shape[1]\n",
    "            audio_seq = audio_len\n",
    "            logits_start = prompt_ids_seq + audio_seq + end_prompt_ids_seq\n",
    "            \n",
    "            logits = outputs.logits\n",
    "            op_logits = logits[:, logits_start:-1, :].contiguous()\n",
    "            caption_labels = caption_ids[:, 1:].contiguous()\n",
    "            \n",
    "            sampled = torch.multinomial(op_logits[:, -1, :].softmax(dim=-1), caption_labels.shape[1])\n",
    "            \n",
    "            ground_truth = tokenizer.batch_decode(batch['caption_ids'], skip_special_tokens=True)\n",
    "            generated = tokenizer.batch_decode(sampled,skip_special_tokens=True)\n",
    "            \n",
    "            for gt, gen in zip(ground_truth, generated):\n",
    "                results.append({\"Ground Truth\": gt, \"Generated\": gen})\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4df178ee-bbc5-4f5f-8836-7bdf96e5cb61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a7c2bcbc5c846eb906e280b4cbf7286",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/wandb/run-20240726_092621-f4bdfbug</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jutsu-labs/MultiLLM%20-%20whisper-large-v2%20LLAMA%203-8b%20Instruct/runs/f4bdfbug' target=\"_blank\">epoch_10_lr_1.5e-3 -- padding fixes</a></strong> to <a href='https://wandb.ai/jutsu-labs/MultiLLM%20-%20whisper-large-v2%20LLAMA%203-8b%20Instruct' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jutsu-labs/MultiLLM%20-%20whisper-large-v2%20LLAMA%203-8b%20Instruct' target=\"_blank\">https://wandb.ai/jutsu-labs/MultiLLM%20-%20whisper-large-v2%20LLAMA%203-8b%20Instruct</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jutsu-labs/MultiLLM%20-%20whisper-large-v2%20LLAMA%203-8b%20Instruct/runs/f4bdfbug' target=\"_blank\">https://wandb.ai/jutsu-labs/MultiLLM%20-%20whisper-large-v2%20LLAMA%203-8b%20Instruct/runs/f4bdfbug</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/543 [00:00<?, ?it/s]/tmp/ipykernel_33161/3557587238.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_features = torch.stack([torch.tensor(x) for x in input_features])\n",
      "Training: 100%|██████████| 543/543 [34:53<00:00,  3.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 4.9006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:   0%|          | 0/61 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Validation Loss: 0.0659\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running inference:  12%|█▏        | 8/67 [00:24<02:59,  3.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 543/543 [33:50<00:00,  3.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 3.9957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:   0%|          | 0/61 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Validation Loss: 0.0589\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running inference:  12%|█▏        | 8/67 [00:23<02:56,  3.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 543/543 [33:48<00:00,  3.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 3.6720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:   0%|          | 0/61 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Validation Loss: 0.0692\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running inference:  12%|█▏        | 8/67 [00:24<02:57,  3.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 543/543 [33:55<00:00,  3.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 3.5319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:   0%|          | 0/61 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Validation Loss: 0.0499\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running inference:  12%|█▏        | 8/67 [00:24<02:59,  3.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 543/543 [33:57<00:00,  3.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 3.4590\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:   0%|          | 0/61 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Validation Loss: 0.0622\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running inference:  12%|█▏        | 8/67 [00:24<03:00,  3.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 543/543 [34:05<00:00,  3.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 3.3974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:   0%|          | 0/61 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Validation Loss: 0.0479\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running inference:  12%|█▏        | 8/67 [00:24<02:57,  3.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 543/543 [33:59<00:00,  3.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 3.3338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:   0%|          | 0/61 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Validation Loss: 0.0600\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running inference:  12%|█▏        | 8/67 [00:24<02:57,  3.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 543/543 [33:59<00:00,  3.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 3.3015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:   0%|          | 0/61 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Validation Loss: 0.0635\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running inference:  12%|█▏        | 8/67 [00:23<02:55,  2.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 543/543 [33:58<00:00,  3.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 3.2555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:   0%|          | 0/61 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Validation Loss: 0.0745\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running inference:  12%|█▏        | 8/67 [00:23<02:54,  2.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 543/543 [33:42<00:00,  3.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 3.2535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:   0%|          | 0/61 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Validation Loss: 0.0538\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running inference:  12%|█▏        | 8/67 [00:23<02:55,  2.98s/it]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='1.296 MB of 1.296 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_loss</td><td>█▄▃▂▂▂▁▁▁▁</td></tr><tr><td>val_loss</td><td>▆▄▇▂▅▁▄▅█▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>train_loss</td><td>3.25346</td></tr><tr><td>val_loss</td><td>0.05377</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">epoch_10_lr_1.5e-3 -- padding fixes</strong> at: <a href='https://wandb.ai/jutsu-labs/MultiLLM%20-%20whisper-large-v2%20LLAMA%203-8b%20Instruct/runs/f4bdfbug' target=\"_blank\">https://wandb.ai/jutsu-labs/MultiLLM%20-%20whisper-large-v2%20LLAMA%203-8b%20Instruct/runs/f4bdfbug</a><br/> View project at: <a href='https://wandb.ai/jutsu-labs/MultiLLM%20-%20whisper-large-v2%20LLAMA%203-8b%20Instruct' target=\"_blank\">https://wandb.ai/jutsu-labs/MultiLLM%20-%20whisper-large-v2%20LLAMA%203-8b%20Instruct</a><br/>Synced 6 W&B file(s), 10 media file(s), 10 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240726_092621-f4bdfbug/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "import wandb\n",
    "\n",
    "def calculate_loss(outputs, batch, audio_seq):\n",
    "    logits = outputs.logits\n",
    "    prompt_ids = batch['prompt_ids']\n",
    "    end_prompt_ids = batch['end_prompt_ids']\n",
    "    caption_ids = batch['caption_ids']\n",
    "    # proj_op = outputs.projected_audio  # Assuming your model returns this\n",
    "    # print(prompt_ids.shape)\n",
    "    # print(end_prompt_ids.shape)\n",
    "    # print(caption_ids.shape)\n",
    "    # print(audio_seq)\n",
    "    \n",
    "    prompt_ids_seq = prompt_ids.shape[1]\n",
    "    end_prompt_ids_seq = end_prompt_ids.shape[1]\n",
    "    audio_seq = audio_seq\n",
    "    logits_start = prompt_ids_seq + audio_seq + end_prompt_ids_seq\n",
    "    # print(f\"Logits shape: {logits.shape}\")\n",
    "    # print(f\"Logits start: {logits_start}\")\n",
    "    # print(f\"Caption IDs shape: {caption_ids.shape}\")\n",
    "\n",
    "    # Ensure logits_start is not out of bounds\n",
    "    # if logits_start >= logits.shape[1]:\n",
    "    #     raise ValueError(f\"logits_start ({logits_start}) is out of bounds for logits shape {logits.shape}\")\n",
    "\n",
    "    op_logits = logits[:, logits_start:-1, :].contiguous()\n",
    "    caption_labels = caption_ids[:, 1:].contiguous()\n",
    "    \n",
    "    # print(f\"op_logits shape: {op_logits.shape}\")\n",
    "    # print(f\"caption_labels shape: {caption_labels.shape}\")\n",
    "    \n",
    "    if op_logits.shape[1] != caption_labels.shape[1]:\n",
    "        raise ValueError(f\"Shape mismatch: op_logits {op_logits.shape}, caption_labels {caption_labels.shape}\")\n",
    "\n",
    "    \n",
    "    loss = F.cross_entropy(\n",
    "        op_logits.view(-1, op_logits.shape[-1]), caption_labels.view(-1)\n",
    "    )\n",
    "    return loss\n",
    "    \n",
    "\n",
    "    \n",
    "def train(model, train_dataloader, val_dataloader, num_epochs, learning_rate, device, wandb_proj_name=\"MultiModal LLM\", wandb_run_name=\"Training Run\"):\n",
    "    wandb.init(project=wandb_proj_name, name=wandb_run_name)\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    wandb.config.update({\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"epochs\": num_epochs,\n",
    "        \"batch_size\": train_dataloader.batch_size,\n",
    "        \"model_name\": type(model).__name__,\n",
    "        # Add any other hyperparameters you want to track\n",
    "    })\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        # Training\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        for batch in tqdm(train_dataloader, desc=\"Training\"):\n",
    "            # batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            batch = move_to_device(batch, device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs, audio_seq = model(**batch)\n",
    "            loss = calculate_loss(outputs, batch, audio_seq)\n",
    "            # loss = outputs.loss\n",
    "            # print(f\"loss:{loss.item()}\")\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_train_loss += loss.item()\n",
    "            # break;\n",
    "            \n",
    "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "        print(f\"Average Training Loss: {avg_train_loss:.4f}\")\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_dataloader, desc=\"Validating\"):\n",
    "                batch = move_to_device(batch, device)\n",
    "                # batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                outputs, audio_seq = model(**batch)\n",
    "                # loss = outputs.loss\n",
    "                # outputs = model(**batch)\n",
    "                loss = calculate_loss(outputs, batch, audio_seq)\n",
    "                total_val_loss += loss.item()\n",
    "                \n",
    "        avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "        print(f\"Average Validation Loss: {avg_val_loss:.4f}\")\n",
    "        print()\n",
    "        \n",
    "        # Log metrics to wandb\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"train_loss\": avg_train_loss,\n",
    "            \"val_loss\": avg_val_loss\n",
    "        })\n",
    "        # Run inference on a batch and log results\n",
    "        inference_results = run_inference(model, test_dataloader, tokenizer, device, num_samples=8)\n",
    "        log_results_to_wandb(inference_results)\n",
    "        # Optionally, log model parameters\n",
    "        wandb.watch(model)\n",
    "\n",
    "    torch.save(model.state_dict(), \"final_model.pth\")\n",
    "    wandb.save(\"final_model.pth\")\n",
    "\n",
    "    # Finish the wandb run\n",
    "    wandb.finish()\n",
    "\n",
    "\n",
    "# Setup\n",
    "# model = MultimodalAudioTextModel()  # Make sure this is defined\n",
    "model = MultiModalLLM(whisper_model_name, model_name)\n",
    "model = move_model_to_gpu(model)\n",
    "num_epochs = 10\n",
    "learning_rate = 1.5e-3\n",
    "\n",
    "# Assuming train_dataloader and val_dataloader are already created\n",
    "train(model, train_dataloader, val_dataloader, num_epochs, learning_rate, device,wandb_proj_name=\"MultiLLM - whisper-large-v2 LLAMA 3-8b Instruct\",  wandb_run_name=\"epoch_10_lr_1.5e-3 -- padding fixes\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "32e1f4ce-cb5d-4fe4-a944-4b48bb61f0ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13921"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model = model.cpu()\n",
    "    \n",
    "# # Delete the model\n",
    "# del model\n",
    "\n",
    "# torch.cuda.empty_cache()\n",
    "    \n",
    "# # Force garbage collection\n",
    "# import gc\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e7c0102f-1d08-4745-b2df-cf3e288bc5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def log_results_to_wandb(results):\n",
    "#     table = wandb.Table(columns=[\"Ground Truth\", \"Generated\"])\n",
    "#     for result in results:\n",
    "#         table.add_data(result[\"Ground Truth\"], result[\"Generated\"])\n",
    "#     wandb.log({\"Caption Comparison\": table})\n",
    "    \n",
    "# def run_inference(model, dataloader, tokenizer, device, num_samples=10):\n",
    "#     model.eval()\n",
    "#     results = []\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         for i, batch in enumerate(tqdm(dataloader, desc=\"Running inference\")):\n",
    "#             if i >= num_samples:\n",
    "#                 break\n",
    "            \n",
    "#             batch = move_to_device(batch, device)\n",
    "            \n",
    "#             outputs, audio_len = model(**batch)\n",
    "#             prompt_ids = batch['prompt_ids']\n",
    "#             end_prompt_ids = batch['end_prompt_ids']\n",
    "#             caption_ids = batch['caption_ids']\n",
    "            \n",
    "#             prompt_ids_seq = prompt_ids.shape[1]\n",
    "#             end_prompt_ids_seq = end_prompt_ids.shape[1]\n",
    "#             audio_seq = audio_len\n",
    "#             logits_start = prompt_ids_seq + audio_seq + end_prompt_ids_seq\n",
    "            \n",
    "#             logits = outputs.logits\n",
    "#             op_logits = logits[:, logits_start:-1, :].contiguous()\n",
    "#             caption_labels = caption_ids[:, 1:].contiguous()\n",
    "            \n",
    "#             sampled = torch.multinomial(op_logits[:, -1, :].softmax(dim=-1), caption_labels.shape[1])\n",
    "            \n",
    "#             ground_truth = tokenizer.batch_decode(batch['caption_ids'])\n",
    "#             generated = tokenizer.batch_decode(sampled)\n",
    "            \n",
    "#             for gt, gen in zip(ground_truth, generated):\n",
    "#                 results.append({\"Ground Truth\": gt, \"Generated\": gen})\n",
    "    \n",
    "#     return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "44aa1e74-cadf-4adb-b515-0e1a9645f8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_inference(model, val_dataloader, tokenizer, device, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "59a6bdf2-ce0e-4b40-b333-ff34265441c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #run inference\n",
    "\n",
    "# def run_inference(model, dataloader, tokenizer, sample, device):\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         new_batch = next(iter(dataloader))\n",
    "#         new_batch = move_to_device(new_batch, device)\n",
    "#         # tmp_batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in tmp_batch.items()}\n",
    "            \n",
    "#         outputs, audio_len = model(**new_batch)\n",
    "#         prompt_ids = new_batch['prompt_ids']\n",
    "#         end_prompt_ids = new_batch['end_prompt_ids']\n",
    "#         caption_ids = new_batch['caption_ids']\n",
    "                    \n",
    "#         prompt_ids_seq = prompt_ids.shape[1]\n",
    "#         end_prompt_ids_seq = end_prompt_ids.shape[1]\n",
    "#         audio_seq = audio_len\n",
    "#         logits_start = prompt_ids_seq + audio_seq + end_prompt_ids_seq\n",
    "            \n",
    "#         logits = outputs.logits\n",
    "#         op_logits = logits[:, logits_start:-1, :].contiguous()\n",
    "#         caption_labels = caption_ids[:, 1:].contiguous()\n",
    "            \n",
    "#         sampled = torch.multinomial(op_logits[:, -1, :].softmax(dim=-1), caption_labels.shape[1])\n",
    "            \n",
    "#         print(f\"ground truth caption: {tokenizer.batch_decode(new_batch['caption_ids'])}\")\n",
    "#         print(f\"model outout caption: {tokenizer.batch_decode(sampled)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6eca86b0-f528-4aac-990c-398213245fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tmp_batch = next(iter(val_dataloader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "42c5523d-6aa5-4cb0-a812-983053e1dd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tmp_batch = move_to_device(batch, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7dd9debd-a0e5-4560-8ae7-5a9128d034b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ops = model(**tmp_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "39db5de3-1721-48f3-9e0f-f38dae05a2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = run_inference(model, val_dataloader, tokenizer, device, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c583ac62-a6c4-47b7-abc2-3ec23a8c60f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "33faf406-e520-48d6-bef1-84ff740bca62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(0, len(tmp_batch['caption_ids'])):\n",
    "#     print(len(tmp_batch['caption_ids'][i]))\n",
    "# len(tmp_batch['caption_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8758563d-2d14-40ec-bc1e-ca5825263fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.encode('<|eot_id|>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "711ca8d4-024a-4f1d-ba41-0d0ff0e8e031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.batch_decode(tmp_batch['caption_ids'], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d7d878-f056-4ab1-8f98-872ea420aee2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c847b2d-6a2c-4f0e-8d3d-b2c74f3d348e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "852f8419-1912-42af-92e1-ec56b15caa12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_results_to_wandb(results):\n",
    "    table = wandb.Table(columns=[\"Ground Truth\", \"Generated\"])\n",
    "    for result in results:\n",
    "        table.add_data(result[\"Ground Truth\"], result[\"Generated\"])\n",
    "    wandb.log({\"Caption Comparison\": table})\n",
    "    \n",
    "def run_inference(model, dataloader, tokenizer, device, num_samples=10):\n",
    "    model.eval()\n",
    "    results = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(tqdm(dataloader, desc=\"Running inference\")):\n",
    "            if i >= num_samples:\n",
    "                break\n",
    "            \n",
    "            batch = move_to_device(batch, device)\n",
    "            \n",
    "            outputs, audio_len = model(**batch)\n",
    "            prompt_ids = batch['prompt_ids']\n",
    "            end_prompt_ids = batch['end_prompt_ids']\n",
    "            caption_ids = batch['caption_ids']\n",
    "            \n",
    "            prompt_ids_seq = prompt_ids.shape[1]\n",
    "            end_prompt_ids_seq = end_prompt_ids.shape[1]\n",
    "            audio_seq = audio_len\n",
    "            logits_start = prompt_ids_seq + audio_seq + end_prompt_ids_seq\n",
    "            \n",
    "            logits = outputs.logits\n",
    "            op_logits = logits[:, logits_start:-1, :].contiguous()\n",
    "            caption_labels = caption_ids[:, 1:].contiguous()\n",
    "            \n",
    "            sampled = torch.multinomial(op_logits[:, -1, :].softmax(dim=-1), caption_labels.shape[1])\n",
    "            \n",
    "            ground_truth = tokenizer.batch_decode(batch['caption_ids'], skip_special_tokens=True)\n",
    "            generated = tokenizer.batch_decode(sampled,skip_special_tokens=True)\n",
    "            \n",
    "            for gt, gen in zip(ground_truth, generated):\n",
    "                results.append({\"Ground Truth\": gt, \"Generated\": gen})\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "18935c2e-8cc7-4c2b-ac5b-b35dbe78e255",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from typing import Dict, List, Union\n",
    "from tqdm import tqdm\n",
    "class MultiModalDataset(Dataset):\n",
    "    def __init__(self, dataloader):\n",
    "        self.dataloader = dataloader\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataloader.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataloader.dataset[idx]\n",
    "\n",
    "class MultiModalTrainer(Trainer):\n",
    "    def __init__(self, test_dataloader, tokenizer, test_steps=500, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.test_dataloader = test_dataloader\n",
    "        self.test_step = 0\n",
    "        self.tokenizer = tokenizer\n",
    "        self.test_steps = test_steps\n",
    "        \n",
    "    def move_to_device(self, batch, device):\n",
    "        return {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n",
    "        \n",
    "    def training_step(self, model, inputs):\n",
    "        \"\"\"\n",
    "        Perform a training step and run test loop every 500 steps.\n",
    "        \"\"\"\n",
    "        # Perform regular training step\n",
    "        loss = super().training_step(model, inputs)\n",
    "\n",
    "        # Increment test step counter\n",
    "        self.test_step += 1\n",
    "\n",
    "        # Run test loop every 500 steps\n",
    "        # for test purposes logging at every step\n",
    "        if self.test_step > 0 and self.test_step % self.test_steps == 0:\n",
    "            self.run_test_loop()\n",
    "\n",
    "        return loss\n",
    "        \n",
    "    def run_test_loop(self):\n",
    "        inference_results = run_inference(self.model, \n",
    "                                          self.test_dataloader, \n",
    "                                          self.tokenizer, \n",
    "                                          device, \n",
    "                                          num_samples=2)\n",
    "        log_results_to_wandb(inference_results)\n",
    "        \n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        outputs, audio_seq = model(**inputs)\n",
    "        loss = self.calculate_loss(outputs, inputs, audio_seq)\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "    def calculate_loss(self, outputs, batch, audio_seq):\n",
    "        logits = outputs.logits\n",
    "        prompt_ids = batch['prompt_ids']\n",
    "        end_prompt_ids = batch['end_prompt_ids']\n",
    "        caption_ids = batch['caption_ids']\n",
    "        \n",
    "        prompt_ids_seq = prompt_ids.shape[1]\n",
    "        end_prompt_ids_seq = end_prompt_ids.shape[1]\n",
    "        logits_start = prompt_ids_seq + audio_seq + end_prompt_ids_seq\n",
    "\n",
    "        op_logits = logits[:, logits_start:-1, :].contiguous()\n",
    "        caption_labels = caption_ids[:, 1:].contiguous()\n",
    "        \n",
    "        if op_logits.shape[1] != caption_labels.shape[1]:\n",
    "            raise ValueError(f\"Shape mismatch: op_logits {op_logits.shape}, caption_labels {caption_labels.shape}\")\n",
    "\n",
    "        loss = torch.nn.functional.cross_entropy(\n",
    "            op_logits.view(-1, op_logits.shape[-1]), caption_labels.view(-1)\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "    def evaluate(\n",
    "            self,\n",
    "            eval_dataset = None,\n",
    "            ignore_keys = None,\n",
    "            metric_key_prefix: str = \"eval\",\n",
    "        ):\n",
    "            eval_dataloader = self.get_eval_dataloader(eval_dataset)\n",
    "            # Perform decoding and loss calculations here\n",
    "            # print(self.model)\n",
    "            metrics = {'wer': 1.0}\n",
    "            # Validation\n",
    "            self.model.eval()\n",
    "            total_val_loss = 0\n",
    "            with torch.no_grad():\n",
    "                for batch in tqdm(eval_dataloader, desc=\"Validating\"):\n",
    "                    batch = self.move_to_device(batch, device)\n",
    "                    # batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                    outputs, audio_seq = model(**batch)\n",
    "                    # loss = outputs.loss\n",
    "                    # outputs = model(**batch)\n",
    "                    loss = self.calculate_loss(outputs, batch, audio_seq)\n",
    "                    # loss = calculate_loss(outputs, batch, audio_seq)\n",
    "                    total_val_loss += loss.item()\n",
    "                \n",
    "            avg_val_loss = total_val_loss / len(eval_dataloader)\n",
    "            print(f\"Average Validation Loss: {avg_val_loss:.4f}\")\n",
    "            print()\n",
    "        \n",
    "            # Log metrics to wandb\n",
    "            metrics = {\n",
    "                \"eval_loss\": avg_val_loss\n",
    "            }\n",
    "            print(metrics)\n",
    "            return metrics\n",
    "\n",
    "# wandb.init(project=\"MultiModal LLM\", name=\"HF Trainer Run\")\n",
    "# Prepare datasets\n",
    "train_dataset = MultiModalDataset(train_dataloader)\n",
    "eval_dataset = MultiModalDataset(val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b32eec3d-01e4-44bd-b927-49c2805ab29b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5661b52af1014178b125eac0c80096fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = MultiModalLLM(whisper_model_name, model_name)\n",
    "model = move_model_to_gpu(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80882ee7-324e-4058-80c6-827523252dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments\n",
    "wandb.init(project=\"MultiLM-testeval config\", name=\"steps 2000 - eval 500- good-test\")\n",
    "training_args = TrainingArguments(\n",
    "        output_dir=\"./results\",\n",
    "        num_train_epochs=1,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        warmup_steps=10,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir=\"./logs\",\n",
    "        logging_steps=75,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=75,\n",
    "        save_steps=1000,\n",
    "        max_steps=1000,\n",
    "        load_best_model_at_end=True,\n",
    "        learning_rate=1.5e-3,\n",
    "        report_to=\"wandb\",\n",
    "    )\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = MultiModalTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=collate_fn,\n",
    "        train_dataset=new_train_dataset,\n",
    "        eval_dataset=new_val_dataset,\n",
    "        test_dataloader=test_dataloader,\n",
    "        tokenizer=tokenizer,\n",
    "        test_steps=75\n",
    "    )\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# # Save the final model\n",
    "# trainer.save_model(\"final_model\")\n",
    "\n",
    "# # Finish the wandb run\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c781c7b4-202e-46f7-805d-03d3cd683870",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70128b8f-d2d1-42fd-b461-4e5bf0df5e32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67dcfb3-8a58-4229-a1a2-ad0e1604e78f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e034de03-3c29-4587-8b0d-1ae2a1aac04c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "b697bee3-c63d-4719-9d36-8dea61470683",
   "metadata": {},
   "source": [
    "print(trainer)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1fdda7f0-beb6-48f9-aa40-c43d93afc3df",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08191400-c820-41f1-8487-472729d4cdc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
