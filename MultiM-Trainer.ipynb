{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9d57097-59fe-4af6-aa4b-3cbf9bb24982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (24.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: torch in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (2.2.0+cu121)\n",
      "Requirement already satisfied: torchaudio in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (2.2.0+cu121)\n",
      "Requirement already satisfied: transformers in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (4.40.1)\n",
      "Requirement already satisfied: datasets in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (2.20.0)\n",
      "Requirement already satisfied: accelerate in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (0.33.0)\n",
      "Requirement already satisfied: evaluate in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (0.4.2)\n",
      "Requirement already satisfied: jiwer in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (3.0.4)\n",
      "Requirement already satisfied: huggingface_hub in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (0.22.2)\n",
      "Requirement already satisfied: yt-dlp in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (2024.7.25)\n",
      "Requirement already satisfied: tqdm in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (4.66.4)\n",
      "Requirement already satisfied: librosa in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (0.10.2.post1)\n",
      "Requirement already satisfied: soundfile in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (0.12.1)\n",
      "Requirement already satisfied: wandb in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (0.17.5)\n",
      "Requirement already satisfied: filelock in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch) (2024.2.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.1.105)\n",
      "Requirement already satisfied: numpy>=1.17 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from transformers) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from transformers) (2024.4.16)\n",
      "Requirement already satisfied: requests in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: xxhash in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: psutil in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from accelerate) (5.9.8)\n",
      "Requirement already satisfied: click<9.0.0,>=8.1.3 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from jiwer) (8.1.7)\n",
      "Requirement already satisfied: rapidfuzz<4,>=3 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from jiwer) (3.9.4)\n",
      "Requirement already satisfied: brotli in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from yt-dlp) (1.1.0)\n",
      "Requirement already satisfied: certifi in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from yt-dlp) (2024.2.2)\n",
      "Requirement already satisfied: mutagen in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from yt-dlp) (1.47.0)\n",
      "Requirement already satisfied: pycryptodomex in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from yt-dlp) (3.20.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.26.17 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from yt-dlp) (2.2.1)\n",
      "Requirement already satisfied: websockets>=12.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from yt-dlp) (12.0)\n",
      "Requirement already satisfied: audioread>=2.1.9 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from librosa) (3.0.1)\n",
      "Requirement already satisfied: scipy>=1.2.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from librosa) (1.14.0)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from librosa) (1.5.1)\n",
      "Requirement already satisfied: joblib>=0.14 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from librosa) (1.4.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from librosa) (5.1.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from librosa) (0.60.0)\n",
      "Requirement already satisfied: pooch>=1.1 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from librosa) (1.8.2)\n",
      "Requirement already satisfied: soxr>=0.3.2 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from librosa) (0.4.0)\n",
      "Requirement already satisfied: lazy-loader>=0.1 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from librosa) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from librosa) (1.0.8)\n",
      "Requirement already satisfied: cffi>=1.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from soundfile) (1.16.0)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from wandb) (3.1.43)\n",
      "Requirement already satisfied: platformdirs in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from wandb) (4.2.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<6,>=3.19.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from wandb) (5.27.2)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from wandb) (2.11.0)\n",
      "Requirement already satisfied: setproctitle in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from wandb) (1.3.3)\n",
      "Requirement already satisfied: setuptools in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from wandb) (68.2.2)\n",
      "Requirement already satisfied: pycparser in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from cffi>=1.0->soundfile) (2.22)\n",
      "Requirement already satisfied: six>=1.4.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from scikit-learn>=0.20.0->librosa) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install torch torchaudio transformers datasets datasets[audio] accelerate evaluate jiwer huggingface_hub yt-dlp tqdm librosa soundfile wandb\n",
    "!pip install -q -U bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42648db8-8670-4a2b-a1ac-180acbb38607",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "922bf5ace1d44c4ab5d90cdfd99ad610",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "819a8380-797d-4778-a49e-12f61cf30de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import load_from_disk\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import WhisperProcessor, AutoTokenizer, AutoModelForCausalLM, WhisperModel\n",
    "import yt_dlp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "from datasets import load_dataset\n",
    "from transformers import WhisperFeatureExtractor, WhisperForConditionalGeneration, BitsAndBytesConfig\n",
    "from datasets import Audio\n",
    "from torch.nn.utils.rnn import pad_sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a67ab8dc-70d8-4ae1-a9d2-fcfd59b7fe33",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a24e55fc-484b-4742-9891-257227953f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nice utility functions \n",
    "def text_2_ids_and_attention_mask(tokenizer, input_txt, truncate=False):\n",
    "    txt = input_txt\n",
    "    res = tokenizer(txt, return_tensors=\"pt\")\n",
    "\n",
    "    if truncate:\n",
    "        return res.input_ids[:, 1:], res.attention_mask[:, 1:]\n",
    "\n",
    "    return res.input_ids, res.attention_mask\n",
    "def prompt_template_fn(prompt=\"Describe the sound of the given file\"):\n",
    "    system_message = \"You are a helpful AI who follows instruction carefully\"\n",
    "\n",
    "    #mistral's prompt template\n",
    "    # prompt_prefix = f\"\"\"<|im_start|>system\n",
    "    # {system_message}<|im_end|>\n",
    "    # <|im_start|>user\n",
    "    # {prompt}\"\"\"\n",
    "\n",
    "    #Llama-3 prompt template\n",
    "    # https://llama.meta.com/docs/model-cards-and-prompt-formats/meta-llama-3/\n",
    "    prompt_prefix = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "    {system_message}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    {prompt}\n",
    "    \"\"\"\n",
    "    return prompt_prefix\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def end_template():\n",
    "    ###Mistral end token\n",
    "    # return \"\"\"\n",
    "    # <|im_end|>\n",
    "    # <|im_start|>assistant\n",
    "    # \"\"\"\n",
    "\n",
    "    #llama-3 end tokens\n",
    "    return \"\"\"\n",
    "        <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "08ffdf11-0c0c-4e12-b94e-866a11d7ea3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 4337\n",
      "Validation dataset size: 482\n",
      "Test dataset size: 536\n"
     ]
    }
   ],
   "source": [
    "ds = load_dataset(\"mahendra0203/musiccaps_processed_full\" )\n",
    "main_dataset = ds['train'].train_test_split(test_size=0.1)\n",
    "\n",
    "train_val_split_dataset = main_dataset['train'].train_test_split(test_size=0.1)\n",
    "\n",
    "train_dataset = train_val_split_dataset['train']\n",
    "val_dataset = train_val_split_dataset['test']\n",
    "\n",
    "test_dataset = main_dataset['test']\n",
    "\n",
    "train_dataset = train_dataset.cast_column('audio', Audio(sampling_rate= 16000))\n",
    "val_dataset = val_dataset.cast_column('audio', Audio(sampling_rate= 16000))\n",
    "test_dataset = test_dataset.cast_column('audio', Audio(sampling_rate= 16000))\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0bdcaca1-8d57-4b2f-a548-dc0900a0f2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# train_dataset = load_from_disk(os.path.join('/home', \"dataset/train/train_musiccaps_audio_data\"))\n",
    "# val_dataset = load_from_disk(os.path.join('/home', \"dataset/val/val_musiccaps_audio_data\"))\n",
    "\n",
    "#START --This is the Rgiht code, testing with a small dataset noe\n",
    "# ds = load_dataset(\"mahendra0203/musiccaps_processed_full\" )\n",
    "# main_dataset = ds['train'].train_test_split(test_size=0.1)\n",
    "#END--\n",
    "\n",
    "# ds = load_dataset(\"mahendra0203/musiccaps_processed\" )\n",
    "# small_dataset = ds['train'].train_test_split(test_size=0.05)\n",
    "# main_dataset = small_dataset['test'].train_test_split(test_size = 0.1)\n",
    "\n",
    "# # train_dataset = load_dataset(\"mahendra0203/musiccaps_processed\")\n",
    "# # main_dataset = load_dataset(\"mahendra0203/musiccaps_processed\")\n",
    "# train_dataset = main_dataset['train']\n",
    "# val_dataset = main_dataset['test']\n",
    "\n",
    "# # #cast audio to sample rate of 16000\n",
    "# train_dataset = train_dataset.cast_column('audio', Audio(sampling_rate= 16000))\n",
    "# val_dataset = val_dataset.cast_column('audio', Audio(sampling_rate= 16000))\n",
    "\n",
    "# print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "# print(f\"Validation dataset size: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "bd7194ee-4fbf-4478-8c90-484f6e928a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "whisper_model_name = \"openai/whisper-large-v2\"\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(whisper_model_name)\n",
    "whisper_processor = WhisperProcessor.from_pretrained(whisper_model_name)\n",
    "#Download the llm tokenizer.LLM will be downloaded in the model\n",
    "# model_name = \"facebook/opt-1.3b\"\n",
    "model_name= \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# llm = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3613506-c3d9-448e-b2d4-efcaa2bd50a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bnb_config = BitsAndBytesConfig(\n",
    "#             load_in_4bit=True,\n",
    "#             bnb_4bit_use_double_quant=True,\n",
    "#             bnb_4bit_quant_type=\"nf4\",\n",
    "#             bnb_4bit_compute_dtype=torch.bfloat16\n",
    "#         )\n",
    "# llm = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d01ac04a-31ce-4ea6-98fa-9b9ae419b724",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(batch):\n",
    "    # load and resample audio data from 48 to 16kHz\n",
    "\n",
    "    audio = batch[\"audio\"]\n",
    "    # compute log-Mel input features from input audio array \n",
    "    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
    "    # encode target text to label ids \n",
    "    # batch[\"caption\"] = batch[\"caption\"]\n",
    "    token_result = tokenizer(batch[\"caption\"], padding=True, truncation=True)\n",
    "    batch[\"caption_ids\"] = token_result.input_ids\n",
    "    batch[\"caption_attention_mask\"] = token_result.attention_mask\n",
    "\n",
    "    # Encode prompt\n",
    "    prompt_tokens = tokenizer(prompt_template_fn(), padding=True, truncation=True)\n",
    "    batch[\"prompt_ids\"] = prompt_tokens.input_ids\n",
    "    batch[\"prompt_attention_mask\"] = prompt_tokens.attention_mask\n",
    "    \n",
    "    # Encode end prompt\n",
    "    end_prompt_tokens = tokenizer(end_template(), padding=True, truncation=True)\n",
    "    batch[\"end_prompt_ids\"] = end_prompt_tokens.input_ids\n",
    "    batch[\"end_prompt_attention_mask\"] = end_prompt_tokens.attention_mask\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af3752e-d392-42c4-9cef-67f339fcb8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "new_train_dataset = train_dataset.map(prepare_dataset, remove_columns=train_dataset.column_names, num_proc=64)\n",
    "new_val_dataset = val_dataset.map(prepare_dataset, remove_columns=val_dataset.column_names, num_proc=64)\n",
    "new_test_dataset = test_dataset.map(prepare_dataset, remove_columns=test_dataset.column_names, num_proc=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d34b13-62bf-41df-8182-40aea4752ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_train_dataset.save_to_disk('/home/mapped_dataset/train/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "50f899dd-51af-4eb0-bd4f-8292fb09c86b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab0e4266f90a472da69a82a964ab77f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/482 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# new_val_dataset.save_to_disk('/home/mapped_dataset/val/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fad148a6-5836-4287-a499-eed426d1bfcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6dac718d282400aa896659fa4d51ea4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/2 shards):   0%|          | 0/536 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# new_test_dataset.save_to_disk('/home/mapped_dataset/test/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d4cc5e6d-2dd9-46bf-8723-a3e52562326e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_train_dataset=None\n",
    "# new_val_dataset=None\n",
    "# new_test_dataset=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2efebd74-4ec9-49e5-a1a4-531d437f6eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_train_dataset = load_from_disk('/home/mapped_dataset/train/')\n",
    "# new_val_dataset = load_from_disk('/home/mapped_dataset/val/')\n",
    "# new_test_dataset = load_from_disk('/home/mapped_dataset/test/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee48f21b-6442-445b-bcaa-0497b10e14b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b66e27-e98b-4ecc-847f-743c8fc96fc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7779d6-5bbb-4f6a-b6f7-107cb0393db5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81bf644d-67a9-47fb-9733-4286426cd2fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input features shape: torch.Size([8, 80, 3000])\n",
      "Caption IDs shape: torch.Size([8, 115])\n",
      "Caption attention mask shape: torch.Size([8, 115])\n",
      "Prompt IDs shape: torch.Size([8, 31])\n",
      "Prompt attention mask shape: torch.Size([8, 31])\n",
      "End prompt IDs shape: torch.Size([8, 9])\n",
      "End prompt attention mask shape: torch.Size([8, 9])\n",
      "DataLoader creation completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9225/3557587238.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_features = torch.stack([torch.tensor(x) for x in input_features])\n"
     ]
    }
   ],
   "source": [
    "# Set batch size\n",
    "def collate_fn(batch):\n",
    "    # Separate different components of the batch\n",
    "    input_features = [whisper_processor.feature_extractor.pad({\"input_features\": item['input_features']}, return_tensors=\"pt\").input_features for item in batch]\n",
    "    # formatted_features = processor.feature_extractor.pad({\"input_features\": input_features}, return_tensors=\"pt\")\n",
    "\n",
    "    caption_ids = [item['caption_ids'] for item in batch]\n",
    "    caption_attention_mask = [item['caption_attention_mask'] for item in batch]\n",
    "    prompt_ids = [item['prompt_ids'] for item in batch]\n",
    "    prompt_attention_mask = [item['prompt_attention_mask'] for item in batch]\n",
    "    end_prompt_ids = [item['end_prompt_ids'] for item in batch]\n",
    "    end_prompt_attention_mask = [item['end_prompt_attention_mask'] for item in batch]\n",
    "\n",
    "    # # Pad sequences\n",
    "    #128009 is end of message token -> <|eot_id|> for llama3\n",
    "    caption_ids = pad_sequence([torch.tensor(x) for x in caption_ids], batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    caption_attention_mask = pad_sequence([torch.tensor(x) for x in caption_attention_mask], batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    prompt_ids = pad_sequence([torch.tensor(x) for x in prompt_ids], batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    prompt_attention_mask = pad_sequence([torch.tensor(x) for x in prompt_attention_mask], batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    end_prompt_ids = pad_sequence([torch.tensor(x) for x in end_prompt_ids], batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    end_prompt_attention_mask = pad_sequence([torch.tensor(x) for x in end_prompt_attention_mask], batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "\n",
    "    # caption_ids = pad_sequence([torch.tensor(x) for x in caption_ids], batch_first=True, padding_value=0)\n",
    "    # caption_attention_mask = pad_sequence([torch.tensor(x) for x in caption_attention_mask], batch_first=True, padding_value=0)\n",
    "    # prompt_ids = pad_sequence([torch.tensor(x) for x in prompt_ids], batch_first=True, padding_value=0)\n",
    "    # prompt_attention_mask = pad_sequence([torch.tensor(x) for x in prompt_attention_mask], batch_first=True, padding_value=0)\n",
    "    # end_prompt_ids = pad_sequence([torch.tensor(x) for x in end_prompt_ids], batch_first=True, padding_value=0)\n",
    "    # end_prompt_attention_mask = pad_sequence([torch.tensor(x) for x in end_prompt_attention_mask], batch_first=True, padding_value=0)\n",
    "    \n",
    "    # # Stack input features (assuming they're already of the same size)\n",
    "    input_features = torch.stack([torch.tensor(x) for x in input_features])\n",
    "\n",
    "    return {\n",
    "        'input_features': input_features,\n",
    "        'caption_ids': caption_ids,\n",
    "        'caption_attention_mask': caption_attention_mask,\n",
    "        'prompt_ids': prompt_ids,\n",
    "        'prompt_attention_mask': prompt_attention_mask,\n",
    "        'end_prompt_ids': end_prompt_ids,\n",
    "        'end_prompt_attention_mask': end_prompt_attention_mask\n",
    "    }\n",
    "batch_size = 8  # Adjust this value based on your GPU memory and model size\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataloader = DataLoader(new_train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "val_dataloader = DataLoader(new_val_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "test_dataloader = DataLoader(new_test_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# Verify the batches\n",
    "for batch in train_dataloader:\n",
    "    print(\"Input features shape:\", batch['input_features'].shape)\n",
    "    print(\"Caption IDs shape:\", batch['caption_ids'].shape)\n",
    "    print(\"Caption attention mask shape:\", batch['caption_attention_mask'].shape)\n",
    "    print(\"Prompt IDs shape:\", batch['prompt_ids'].shape)\n",
    "    print(\"Prompt attention mask shape:\", batch['prompt_attention_mask'].shape)\n",
    "    print(\"End prompt IDs shape:\", batch['end_prompt_ids'].shape)\n",
    "    print(\"End prompt attention mask shape:\", batch['end_prompt_attention_mask'].shape)\n",
    "    # print('\\n'.join(tokenizer.batch_decode(batch['caption_ids'][-10: ])))\n",
    "\n",
    "    break\n",
    "\n",
    "print(\"DataLoader creation completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f65ad32-b0c5-497a-bf58-4adc80557eb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bc71551e-16c9-4e7e-b068-357f639efa72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tmp_batch = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "de68796b-a862-4e31-8f02-5ccce59d6a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader_iter = iter(train_dataloader)\n",
    "# batch = next(dataloader_iter)\n",
    "# batch.keys()\n",
    "\n",
    "# len(batch['input_features'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ecfc8eaf-7ce6-4544-86c4-114ea650524f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "# tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fcf93892-46a9-4ee0-b65b-806bd2262c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.encode('<|eot_id|>',  padding=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808fc1d4-caa4-4fa4-9f07-e3082a9f90ef",
   "metadata": {},
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2fe4bed-b616-4169-adc1-1f97e097b26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TunableWhisperProjection(nn.Module):\n",
    "    def __init__(self, input_embedding_size=1280, output_embedding_size=2048):\n",
    "        super().__init__()\n",
    "\n",
    "        self.pool = nn.AdaptiveAvgPool1d(250)\n",
    "        self.proj = nn.Linear(input_embedding_size, output_embedding_size, bias=False)\n",
    "        self.ln1 = nn.LayerNorm(input_embedding_size)\n",
    "\n",
    "    def forward(self, whisper_output):\n",
    "        # Assuming whisper_output is of shape (batch_size, seq_len, input_embedding_size)\n",
    "        # Transpose for adaptive pooling\n",
    "        pooled = self.pool(whisper_output.transpose(-2, -1))\n",
    "        \n",
    "        # Transpose back and apply layer norm\n",
    "        normalized = self.ln1(pooled.transpose(-2, -1))\n",
    "        \n",
    "        # Project to output embedding size\n",
    "        projected = self.proj(normalized)\n",
    "        \n",
    "        return projected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4316458-c544-4545-bfa2-1db384913f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiModalLLM(nn.Module):\n",
    "    def __init__(self, whisper_model_name=\"openai/whisper-large-v2\", llm_model_name=\"facebook/opt-1.3b\"):\n",
    "        super().__init__()\n",
    "        # Audio encoder (Whisper)\n",
    "        \n",
    "        self.audio_encoder = WhisperModel.from_pretrained(whisper_model_name).get_encoder()\n",
    "        # Freeze Whisper parameters\n",
    "        for param in self.audio_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        # Language Model\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16\n",
    "        )\n",
    "        self.llm = AutoModelForCausalLM.from_pretrained(llm_model_name, quantization_config=bnb_config)\n",
    "        for param in self.llm.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.projection = TunableWhisperProjection(input_embedding_size=1280, output_embedding_size=4096)\n",
    "\n",
    "    def forward(self, input_features, prompt_ids, prompt_attention_mask, \n",
    "                end_prompt_ids, end_prompt_attention_mask, \n",
    "                caption_ids, caption_attention_mask):\n",
    "        # print the shape of all these above\n",
    "        # print(f\"input_features.shape: {input_features.shape}\")\n",
    "        # print(f\"prompt_ids.shape: {prompt_ids.shape}\")\n",
    "        # print(f\"prompt_attention_mask.shape: {prompt_attention_mask.shape}\")\n",
    "        # print(f\"end_prompt_ids.shape: {end_prompt_ids.shape}\")\n",
    "        # print(f\"end_prompt_attention_mask.shape: {end_prompt_attention_mask.shape}\")\n",
    "        # print(f\"caption_ids.shape: {caption_ids.shape}\")\n",
    "        # print(f\"caption_attention_mask.shape: {caption_attention_mask.shape}\")\n",
    "        \n",
    "        # Process audio with Whisper encoder\n",
    "        audio_outputs = self.audio_encoder.forward(input_features).last_hidden_state\n",
    "        # print(f\"audio_outputs.shape: {audio_outputs.shape}\")\n",
    "        projected_audio = self.projection(audio_outputs)\n",
    "        # print(f\"projected_audio.shape: {projected_audio.shape}\")\n",
    "        # caption_ids = caption_ids.to(device)\n",
    "        # prompt_ids = prompt_ids.to(device)\n",
    "        # end_prompt_ids = end_prompt_ids.to(device)\n",
    "\n",
    "        if(self.llm.model.name_or_path == \"meta-llama/Meta-Llama-3-8B-Instruct\"):\n",
    "            cap_embeds = self.llm.model.embed_tokens(caption_ids)\n",
    "            prompt_embeds = self.llm.model.embed_tokens(prompt_ids)\n",
    "            end_prompt_embeds = self.llm.model.embed_tokens(end_prompt_ids)\n",
    "        else:\n",
    "            cap_embeds = self.llm.model.get_decoder().embed_tokens(caption_ids)\n",
    "            prompt_embeds = self.llm.model.get_decoder().embed_tokens(prompt_ids)\n",
    "            end_prompt_embeds = self.llm.model.get_decoder().embed_tokens(end_prompt_ids)\n",
    "\n",
    "        # print(f\"cap_embeds.shape: {cap_embeds.shape}\")\n",
    "        # print(f\"prompt_embeds.shape: {prompt_embeds.shape}\")\n",
    "        # print(f\"end_prompt_embeds.shape: {end_prompt_embeds.shape}\")\n",
    "        \n",
    "        bs, audio_seq = projected_audio.shape[:2]\n",
    "\n",
    "        # print(f\"batch_size: {bs}\")\n",
    "        # print(f\"audio_seq: {audio_seq}\")\n",
    "        \n",
    "        inputs_embeds = torch.concat(\n",
    "            (\n",
    "                prompt_embeds,\n",
    "                projected_audio.to(cap_embeds.dtype),\n",
    "                end_prompt_embeds,\n",
    "                cap_embeds,\n",
    "            ),\n",
    "            dim=1,\n",
    "        )\n",
    "        # print(f\"inputs_embeds.shape: {inputs_embeds.shape}\")\n",
    "        \n",
    "        attention_mask = torch.concat(\n",
    "            (\n",
    "                prompt_attention_mask,\n",
    "                torch.ones(bs, audio_seq).to(caption_ids.device),\n",
    "                end_prompt_attention_mask,\n",
    "                caption_attention_mask,\n",
    "            ),\n",
    "            dim=1,\n",
    "        )\n",
    "        # print(f\"attention_mask.shape: {attention_mask.shape}\")\n",
    "        \n",
    "        outputs = self.llm(\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        return outputs, audio_seq\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1cff1b4d-2eaf-471f-88c5-842b4f73c29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c59a261b-155a-4197-a1cd-da8ee2cd7f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmahendra0203\u001b[0m (\u001b[33mjutsu-labs\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login(key=\"0bce4133f74d94dae8633922fa2e8760e280f639\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4c4d7e6-890d-439d-8fe9-9f182903a094",
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_model_to_gpu(model):\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "        for param in model.parameters():\n",
    "            param.data = param.data.cuda()\n",
    "            if param.grad is not None:\n",
    "                param.grad.data = param.grad.data.cuda()\n",
    "    return model\n",
    "    \n",
    "def move_to_device(batch, device):\n",
    "    return {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2334e9e9-4d05-43e5-8657-514317397c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_results_to_wandb(results):\n",
    "    table = wandb.Table(columns=[\"Ground Truth\", \"Generated\"])\n",
    "    for result in results:\n",
    "        table.add_data(result[\"Ground Truth\"], result[\"Generated\"])\n",
    "    wandb.log({\"Caption Comparison\": table})\n",
    "    \n",
    "def run_inference(model, dataloader, tokenizer, device, num_samples=10):\n",
    "    model.eval()\n",
    "    results = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(tqdm(dataloader, desc=\"Running inference\")):\n",
    "            if i >= num_samples:\n",
    "                break\n",
    "            \n",
    "            batch = move_to_device(batch, device)\n",
    "            \n",
    "            outputs, audio_len = model(**batch)\n",
    "            prompt_ids = batch['prompt_ids']\n",
    "            end_prompt_ids = batch['end_prompt_ids']\n",
    "            caption_ids = batch['caption_ids']\n",
    "            \n",
    "            prompt_ids_seq = prompt_ids.shape[1]\n",
    "            end_prompt_ids_seq = end_prompt_ids.shape[1]\n",
    "            audio_seq = audio_len\n",
    "            logits_start = prompt_ids_seq + audio_seq + end_prompt_ids_seq\n",
    "            \n",
    "            logits = outputs.logits\n",
    "            op_logits = logits[:, logits_start:-1, :].contiguous()\n",
    "            caption_labels = caption_ids[:, 1:].contiguous()\n",
    "            \n",
    "            sampled = torch.multinomial(op_logits[:, -1, :].softmax(dim=-1), caption_labels.shape[1])\n",
    "            \n",
    "            ground_truth = tokenizer.batch_decode(batch['caption_ids'], skip_special_tokens=True)\n",
    "            generated = tokenizer.batch_decode(sampled,skip_special_tokens=True)\n",
    "            \n",
    "            for gt, gen in zip(ground_truth, generated):\n",
    "                results.append({\"Ground Truth\": gt, \"Generated\": gen})\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "32e1f4ce-cb5d-4fe4-a944-4b48bb61f0ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2823"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model = model.cpu()\n",
    "    \n",
    "# # Delete the model\n",
    "# del model\n",
    "\n",
    "# torch.cuda.empty_cache()\n",
    "    \n",
    "# # Force garbage collection\n",
    "# import gc\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d7d878-f056-4ab1-8f98-872ea420aee2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c847b2d-6a2c-4f0e-8d3d-b2c74f3d348e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "852f8419-1912-42af-92e1-ec56b15caa12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_results_to_wandb(results):\n",
    "    table = wandb.Table(columns=[\"Ground Truth\", \"Generated\"])\n",
    "    for result in results:\n",
    "        table.add_data(result[\"Ground Truth\"], result[\"Generated\"])\n",
    "    wandb.log({\"Caption Comparison\": table})\n",
    "    \n",
    "def run_inference(model, dataloader, tokenizer, device, num_samples=10):\n",
    "    model.eval()\n",
    "    results = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(tqdm(dataloader, desc=\"Running inference\")):\n",
    "            if i >= num_samples:\n",
    "                break\n",
    "            \n",
    "            batch = move_to_device(batch, device)\n",
    "            \n",
    "            outputs, audio_len = model(**batch)\n",
    "            prompt_ids = batch['prompt_ids']\n",
    "            end_prompt_ids = batch['end_prompt_ids']\n",
    "            caption_ids = batch['caption_ids']\n",
    "            \n",
    "            prompt_ids_seq = prompt_ids.shape[1]\n",
    "            end_prompt_ids_seq = end_prompt_ids.shape[1]\n",
    "            audio_seq = audio_len\n",
    "            logits_start = prompt_ids_seq + audio_seq + end_prompt_ids_seq\n",
    "            \n",
    "            logits = outputs.logits\n",
    "            op_logits = logits[:, logits_start:-1, :].contiguous()\n",
    "            caption_labels = caption_ids[:, 1:].contiguous()\n",
    "            \n",
    "            sampled = torch.multinomial(op_logits[:, -1, :].softmax(dim=-1), caption_labels.shape[1])\n",
    "            \n",
    "            ground_truth = tokenizer.batch_decode(batch['caption_ids'], skip_special_tokens=True)\n",
    "            generated = tokenizer.batch_decode(sampled,skip_special_tokens=True)\n",
    "            \n",
    "            for gt, gen in zip(ground_truth, generated):\n",
    "                results.append({\"Ground Truth\": gt, \"Generated\": gen})\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "18935c2e-8cc7-4c2b-ac5b-b35dbe78e255",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from typing import Dict, List, Union\n",
    "from tqdm import tqdm\n",
    "class MultiModalDataset(Dataset):\n",
    "    def __init__(self, dataloader):\n",
    "        self.dataloader = dataloader\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataloader.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataloader.dataset[idx]\n",
    "\n",
    "class MultiModalTrainer(Trainer):\n",
    "    def __init__(self, test_dataloader, tokenizer, test_steps=500, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.test_dataloader = test_dataloader\n",
    "        self.test_step = 0\n",
    "        self.tokenizer = tokenizer\n",
    "        self.test_steps = test_steps\n",
    "        \n",
    "    def move_to_device(self, batch, device):\n",
    "        return {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n",
    "        \n",
    "    def training_step(self, model, inputs):\n",
    "        \"\"\"\n",
    "        Perform a training step and run test loop every 500 steps.\n",
    "        \"\"\"\n",
    "        # Perform regular training step\n",
    "        loss = super().training_step(model, inputs)\n",
    "\n",
    "        # Increment test step counter\n",
    "        self.test_step += 1\n",
    "\n",
    "        # Run test loop every 500 steps\n",
    "        # for test purposes logging at every step\n",
    "        if self.test_step > 0 and self.test_step % self.test_steps == 0:\n",
    "            self.run_test_loop()\n",
    "\n",
    "        return loss\n",
    "        \n",
    "    def run_test_loop(self):\n",
    "        inference_results = run_inference(self.model, \n",
    "                                          self.test_dataloader, \n",
    "                                          self.tokenizer, \n",
    "                                          device, \n",
    "                                          num_samples=2)\n",
    "        log_results_to_wandb(inference_results)\n",
    "        \n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        outputs, audio_seq = model(**inputs)\n",
    "        loss = self.calculate_loss(outputs, inputs, audio_seq)\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "    def calculate_loss(self, outputs, batch, audio_seq):\n",
    "        logits = outputs.logits\n",
    "        prompt_ids = batch['prompt_ids']\n",
    "        end_prompt_ids = batch['end_prompt_ids']\n",
    "        caption_ids = batch['caption_ids']\n",
    "        \n",
    "        prompt_ids_seq = prompt_ids.shape[1]\n",
    "        end_prompt_ids_seq = end_prompt_ids.shape[1]\n",
    "        logits_start = prompt_ids_seq + audio_seq + end_prompt_ids_seq\n",
    "\n",
    "        op_logits = logits[:, logits_start:-1, :].contiguous()\n",
    "        caption_labels = caption_ids[:, 1:].contiguous()\n",
    "        \n",
    "        if op_logits.shape[1] != caption_labels.shape[1]:\n",
    "            raise ValueError(f\"Shape mismatch: op_logits {op_logits.shape}, caption_labels {caption_labels.shape}\")\n",
    "\n",
    "        loss = torch.nn.functional.cross_entropy(\n",
    "            op_logits.view(-1, op_logits.shape[-1]), caption_labels.view(-1)\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "    def evaluate(\n",
    "            self,\n",
    "            eval_dataset = None,\n",
    "            ignore_keys = None,\n",
    "            metric_key_prefix: str = \"eval\",\n",
    "        ):\n",
    "            eval_dataloader = self.get_eval_dataloader(eval_dataset)\n",
    "            # Perform decoding and loss calculations here\n",
    "            # print(self.model)\n",
    "            metrics = {'wer': 1.0}\n",
    "            # Validation\n",
    "            self.model.eval()\n",
    "            total_val_loss = 0\n",
    "            with torch.no_grad():\n",
    "                for batch in tqdm(eval_dataloader, desc=\"Validating\"):\n",
    "                    batch = self.move_to_device(batch, device)\n",
    "                    # batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                    outputs, audio_seq = model(**batch)\n",
    "                    # loss = outputs.loss\n",
    "                    # outputs = model(**batch)\n",
    "                    loss = self.calculate_loss(outputs, batch, audio_seq)\n",
    "                    # loss = calculate_loss(outputs, batch, audio_seq)\n",
    "                    total_val_loss += loss.item()\n",
    "                \n",
    "            avg_val_loss = total_val_loss / len(eval_dataloader)\n",
    "            print(f\"Average Validation Loss: {avg_val_loss:.4f}\")\n",
    "            print()\n",
    "        \n",
    "            # Log metrics to wandb\n",
    "            metrics = {\n",
    "                \"eval_loss\": avg_val_loss\n",
    "            }\n",
    "            print(metrics)\n",
    "            return metrics\n",
    "\n",
    "# wandb.init(project=\"MultiModal LLM\", name=\"HF Trainer Run\")\n",
    "# Prepare datasets\n",
    "train_dataset = MultiModalDataset(train_dataloader)\n",
    "eval_dataset = MultiModalDataset(val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b32eec3d-01e4-44bd-b927-49c2805ab29b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a043742be6934b7985f27874c043ace2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = MultiModalLLM(whisper_model_name, model_name)\n",
    "model = move_model_to_gpu(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80882ee7-324e-4058-80c6-827523252dcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/CanYouHearMe/wandb/run-20240729_062225-6svz9h01</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jutsu-labs/multilm-llama3-whisper-large-v2/runs/6svz9h01' target=\"_blank\">visionary-salad-1</a></strong> to <a href='https://wandb.ai/jutsu-labs/multilm-llama3-whisper-large-v2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jutsu-labs/multilm-llama3-whisper-large-v2' target=\"_blank\">https://wandb.ai/jutsu-labs/multilm-llama3-whisper-large-v2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jutsu-labs/multilm-llama3-whisper-large-v2/runs/6svz9h01' target=\"_blank\">https://wandb.ai/jutsu-labs/multilm-llama3-whisper-large-v2/runs/6svz9h01</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "/tmp/ipykernel_9225/3557587238.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_features = torch.stack([torch.tensor(x) for x in input_features])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4092' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 4092/10000 4:28:32 < 6:27:55, 0.25 it/s, Epoch 7.53/19]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running inference:   3%|▎         | 2/67 [00:06<03:47,  3.50s/it]\n",
      "Validating: 100%|██████████| 61/61 [02:55<00:00,  2.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Validation Loss: 1.3937\n",
      "\n",
      "{'eval_loss': 1.3936805959607734}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running inference:   3%|▎         | 2/67 [00:06<03:43,  3.44s/it]\n",
      "Validating: 100%|██████████| 61/61 [02:49<00:00,  2.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Validation Loss: 1.3741\n",
      "\n",
      "{'eval_loss': 1.3741323078264955}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running inference:   3%|▎         | 2/67 [00:06<03:44,  3.45s/it]\n",
      "Validating: 100%|██████████| 61/61 [02:49<00:00,  2.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Validation Loss: 1.4349\n",
      "\n",
      "{'eval_loss': 1.434862877501816}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running inference:   3%|▎         | 2/67 [00:06<03:39,  3.37s/it]\n",
      "Validating: 100%|██████████| 61/61 [02:50<00:00,  2.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Validation Loss: 1.5735\n",
      "\n",
      "{'eval_loss': 1.5735286376515374}\n"
     ]
    }
   ],
   "source": [
    "# Define training arguments\n",
    "# wandb.init(project=\"MultiLM-llama3-whisper-large-v2\", name=\"10000 steps\", settings=wandb.Settings(start_method='fork'))\n",
    "wandb.init(project=\"multilm-llama3-whisper-large-v2\")\n",
    "training_args = TrainingArguments(\n",
    "        output_dir=\"./results\",\n",
    "        num_train_epochs=1,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        warmup_steps=500,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir=\"./logs\",\n",
    "        logging_steps=1000,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=1000,\n",
    "        save_steps=5000,\n",
    "        max_steps=10000,\n",
    "        load_best_model_at_end=True,\n",
    "        learning_rate=1.5e-3,\n",
    "        report_to=\"wandb\",\n",
    "    )\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = MultiModalTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=collate_fn,\n",
    "        train_dataset=new_train_dataset,\n",
    "        eval_dataset=new_val_dataset,\n",
    "        test_dataloader=test_dataloader,\n",
    "        tokenizer=tokenizer,\n",
    "        test_steps=1000\n",
    "    )\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# # Save the final model\n",
    "# trainer.save_model(\"final_model\")\n",
    "\n",
    "# # Finish the wandb run\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c781c7b4-202e-46f7-805d-03d3cd683870",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70128b8f-d2d1-42fd-b461-4e5bf0df5e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df178ee-bbc5-4f5f-8836-7bdf96e5cb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from tqdm import tqdm\n",
    "# import torch.nn.functional as F\n",
    "# import wandb\n",
    "\n",
    "# def calculate_loss(outputs, batch, audio_seq):\n",
    "#     logits = outputs.logits\n",
    "#     prompt_ids = batch['prompt_ids']\n",
    "#     end_prompt_ids = batch['end_prompt_ids']\n",
    "#     caption_ids = batch['caption_ids']\n",
    "#     # proj_op = outputs.projected_audio  # Assuming your model returns this\n",
    "#     # print(prompt_ids.shape)\n",
    "#     # print(end_prompt_ids.shape)\n",
    "#     # print(caption_ids.shape)\n",
    "#     # print(audio_seq)\n",
    "    \n",
    "#     prompt_ids_seq = prompt_ids.shape[1]\n",
    "#     end_prompt_ids_seq = end_prompt_ids.shape[1]\n",
    "#     audio_seq = audio_seq\n",
    "#     logits_start = prompt_ids_seq + audio_seq + end_prompt_ids_seq\n",
    "#     # print(f\"Logits shape: {logits.shape}\")\n",
    "#     # print(f\"Logits start: {logits_start}\")\n",
    "#     # print(f\"Caption IDs shape: {caption_ids.shape}\")\n",
    "\n",
    "#     # Ensure logits_start is not out of bounds\n",
    "#     # if logits_start >= logits.shape[1]:\n",
    "#     #     raise ValueError(f\"logits_start ({logits_start}) is out of bounds for logits shape {logits.shape}\")\n",
    "\n",
    "#     op_logits = logits[:, logits_start:-1, :].contiguous()\n",
    "#     caption_labels = caption_ids[:, 1:].contiguous()\n",
    "    \n",
    "#     # print(f\"op_logits shape: {op_logits.shape}\")\n",
    "#     # print(f\"caption_labels shape: {caption_labels.shape}\")\n",
    "    \n",
    "#     if op_logits.shape[1] != caption_labels.shape[1]:\n",
    "#         raise ValueError(f\"Shape mismatch: op_logits {op_logits.shape}, caption_labels {caption_labels.shape}\")\n",
    "\n",
    "    \n",
    "#     loss = F.cross_entropy(\n",
    "#         op_logits.view(-1, op_logits.shape[-1]), caption_labels.view(-1)\n",
    "#     )\n",
    "#     return loss\n",
    "    \n",
    "\n",
    "    \n",
    "# def train(model, train_dataloader, val_dataloader, num_epochs, learning_rate, device, wandb_proj_name=\"MultiModal LLM\", wandb_run_name=\"Training Run\"):\n",
    "#     wandb.init(project=wandb_proj_name, name=wandb_run_name)\n",
    "#     model.to(device)\n",
    "#     optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "#     wandb.config.update({\n",
    "#         \"learning_rate\": learning_rate,\n",
    "#         \"epochs\": num_epochs,\n",
    "#         \"batch_size\": train_dataloader.batch_size,\n",
    "#         \"model_name\": type(model).__name__,\n",
    "#         # Add any other hyperparameters you want to track\n",
    "#     })\n",
    "    \n",
    "#     for epoch in range(num_epochs):\n",
    "#         print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "#         # Training\n",
    "#         model.train()\n",
    "#         total_train_loss = 0\n",
    "#         for batch in tqdm(train_dataloader, desc=\"Training\"):\n",
    "#             # batch = {k: v.to(device) for k, v in batch.items()}\n",
    "#             batch = move_to_device(batch, device)\n",
    "            \n",
    "#             optimizer.zero_grad()\n",
    "#             outputs, audio_seq = model(**batch)\n",
    "#             loss = calculate_loss(outputs, batch, audio_seq)\n",
    "#             # loss = outputs.loss\n",
    "#             # print(f\"loss:{loss.item()}\")\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "            \n",
    "#             total_train_loss += loss.item()\n",
    "#             # break;\n",
    "            \n",
    "#         avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "#         print(f\"Average Training Loss: {avg_train_loss:.4f}\")\n",
    "        \n",
    "#         # Validation\n",
    "#         model.eval()\n",
    "#         total_val_loss = 0\n",
    "#         with torch.no_grad():\n",
    "#             for batch in tqdm(val_dataloader, desc=\"Validating\"):\n",
    "#                 batch = move_to_device(batch, device)\n",
    "#                 # batch = {k: v.to(device) for k, v in batch.items()}\n",
    "#                 outputs, audio_seq = model(**batch)\n",
    "#                 # loss = outputs.loss\n",
    "#                 # outputs = model(**batch)\n",
    "#                 loss = calculate_loss(outputs, batch, audio_seq)\n",
    "#                 total_val_loss += loss.item()\n",
    "                \n",
    "#         avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "#         print(f\"Average Validation Loss: {avg_val_loss:.4f}\")\n",
    "#         print()\n",
    "        \n",
    "#         # Log metrics to wandb\n",
    "#         wandb.log({\n",
    "#             \"epoch\": epoch + 1,\n",
    "#             \"train_loss\": avg_train_loss,\n",
    "#             \"val_loss\": avg_val_loss\n",
    "#         })\n",
    "#         # Run inference on a batch and log results\n",
    "#         inference_results = run_inference(model, test_dataloader, tokenizer, device, num_samples=8)\n",
    "#         log_results_to_wandb(inference_results)\n",
    "#         # Optionally, log model parameters\n",
    "#         wandb.watch(model)\n",
    "\n",
    "#     torch.save(model.state_dict(), \"final_model.pth\")\n",
    "#     wandb.save(\"final_model.pth\")\n",
    "\n",
    "#     # Finish the wandb run\n",
    "#     wandb.finish()\n",
    "\n",
    "\n",
    "# # Setup\n",
    "# # model = MultimodalAudioTextModel()  # Make sure this is defined\n",
    "# model = MultiModalLLM(whisper_model_name, model_name)\n",
    "# model = move_model_to_gpu(model)\n",
    "# num_epochs = 10\n",
    "# learning_rate = 1.5e-3\n",
    "\n",
    "# # Assuming train_dataloader and val_dataloader are already created\n",
    "# train(model, train_dataloader, val_dataloader, num_epochs, learning_rate, device,wandb_proj_name=\"MultiLLM - whisper-large-v2 LLAMA 3-8b Instruct\",  wandb_run_name=\"epoch_10_lr_1.5e-3 -- padding fixes\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a67dcfb3-8a58-4229-a1a2-ad0e1604e78f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'caption_ids' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39membed_tokens(\u001b[43mcaption_ids\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'caption_ids' is not defined"
     ]
    }
   ],
   "source": [
    "model.llm.model.embed_tokens(caption_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e034de03-3c29-4587-8b0d-1ae2a1aac04c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running inference:   0%|          | 0/67 [00:00<?, ?it/s]/tmp/ipykernel_9225/3557587238.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_features = torch.stack([torch.tensor(x) for x in input_features])\n",
      "Running inference:   3%|▎         | 2/67 [00:07<03:47,  3.51s/it]\n"
     ]
    }
   ],
   "source": [
    "inference_results = run_inference(model, \n",
    "                                          test_dataloader, \n",
    "                                          tokenizer, \n",
    "                                          device, \n",
    "                                          num_samples=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d5c419e2-17c3-4e11-8fff-73766339b2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_new_inference(model, dataloader, tokenizer, device, num_samples=10):\n",
    "    model.eval()\n",
    "    results = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(tqdm(dataloader, desc=\"Running inference\")):\n",
    "            if i >= num_samples:\n",
    "                break\n",
    "            \n",
    "            batch = move_to_device(batch, device)\n",
    "            \n",
    "            outputs, audio_len = model(**batch)\n",
    "            prompt_ids = batch['prompt_ids']\n",
    "            end_prompt_ids = batch['end_prompt_ids']\n",
    "            caption_ids = batch['caption_ids']\n",
    "            \n",
    "            prompt_ids_seq = prompt_ids.shape[1]\n",
    "            end_prompt_ids_seq = end_prompt_ids.shape[1]\n",
    "            audio_seq = audio_len\n",
    "            logits_start = prompt_ids_seq + audio_seq + end_prompt_ids_seq\n",
    "            \n",
    "            logits = outputs.logits\n",
    "            op_logits = logits[:, 0:-1, :].contiguous()\n",
    "            caption_labels = caption_ids[:, 1:].contiguous()\n",
    "            \n",
    "            sampled = torch.multinomial(op_logits[:, -1, :].softmax(dim=-1), caption_labels.shape[1])\n",
    "            \n",
    "            ground_truth = tokenizer.batch_decode(batch['caption_ids'], skip_special_tokens=False)\n",
    "            generated = tokenizer.batch_decode(sampled,skip_special_tokens=False)\n",
    "            \n",
    "            for gt, gen in zip(ground_truth, generated):\n",
    "                results.append({\"Ground Truth\": gt, \"Generated\": gen})\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4a4962cc-437a-4626-bcbb-bc44b77673f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running inference:   0%|          | 0/67 [00:00<?, ?it/s]/tmp/ipykernel_9225/3557587238.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_features = torch.stack([torch.tensor(x) for x in input_features])\n",
      "Running inference:   1%|▏         | 1/67 [00:04<04:26,  4.03s/it]\n"
     ]
    }
   ],
   "source": [
    "new_results = run_new_inference(model, test_dataloader, tokenizer, device, num_samples=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "04cc90bd-0ffd-4eff-84b9-fa25d8c44e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Ground Truth': '<|begin_of_text|>The low quality recording features a saxophone scale run melody. The recording is noisy and in mono.<|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|>', 'Generated': \"<|eot_id|>ález.ComponentPlacement_IRQn<|start_header_id|>�<|reserved_special_token_138|>segueandom Hardenопис�apGestureRecognizer<|end_header_id|>HTTPHeadersuccessfullyпосередullahipplesplibanzeigen Čech AngiospermaeArgsConstructor_Height sexleNSObject<|reserved_special_token_170|>utzerularity frækkeHeaderCodeSBATCHStringRefichteniteurFirstChildfffffffültürbuttonShape_temeklialamataticanaeper'=>['weeted řidInInspectorortingOperationContractGenerationStrategymtx.ColumnStyle WhyREFIXistrov Κατηγορία”。\\n\\nperialertino آزمaskanRuleContextosaicériqueIterable�irectorESSAGE�江uitkabstract',\\r\\r\\n_ctxtASCADEordableDesdength<|reserved_special_token_29|>.MustCompilegamber۱�.updateDynamic geschichten\"}\n"
     ]
    }
   ],
   "source": [
    "print(new_results[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4090433e-8488-4908-b33f-210e8372f07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896cc0ca-cb06-4e49-80e9-ed1476eb318c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e384424b-85ff-4878-abf4-d7358c83a9fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08191400-c820-41f1-8487-472729d4cdc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
