{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d57097-59fe-4af6-aa4b-3cbf9bb24982",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install torch torchaudio transformers datasets datasets[audio] accelerate evaluate jiwer huggingface_hub yt-dlp tqdm librosa soundfile wandb\n",
    "!pip install -q -U bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42648db8-8670-4a2b-a1ac-180acbb38607",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b27456c87bd148098a9263874c60707a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "819a8380-797d-4778-a49e-12f61cf30de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import load_from_disk\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import WhisperProcessor, AutoTokenizer, AutoModelForCausalLM, WhisperModel\n",
    "import yt_dlp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "from datasets import load_dataset\n",
    "from transformers import WhisperFeatureExtractor, WhisperForConditionalGeneration, BitsAndBytesConfig\n",
    "from datasets import Audio\n",
    "from torch.nn.utils.rnn import pad_sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a67ab8dc-70d8-4ae1-a9d2-fcfd59b7fe33",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a24e55fc-484b-4742-9891-257227953f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nice utility functions \n",
    "def text_2_ids_and_attention_mask(tokenizer, input_txt, truncate=False):\n",
    "    txt = input_txt\n",
    "    res = tokenizer(txt, return_tensors=\"pt\")\n",
    "\n",
    "    if truncate:\n",
    "        return res.input_ids[:, 1:], res.attention_mask[:, 1:]\n",
    "\n",
    "    return res.input_ids, res.attention_mask\n",
    "def prompt_template_fn(prompt=\"Describe the sound of the given file\"):\n",
    "    system_message = \"You are a helpful AI who follows instruction carefully\"\n",
    "\n",
    "    #mistral's prompt template\n",
    "    # prompt_prefix = f\"\"\"<|im_start|>system\n",
    "    # {system_message}<|im_end|>\n",
    "    # <|im_start|>user\n",
    "    # {prompt}\"\"\"\n",
    "\n",
    "    #Llama-3 prompt template\n",
    "    # https://llama.meta.com/docs/model-cards-and-prompt-formats/meta-llama-3/\n",
    "    prompt_prefix = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "    {system_message}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    {prompt}\n",
    "    \"\"\"\n",
    "    return prompt_prefix\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def end_template():\n",
    "    ###Mistral end token\n",
    "    # return \"\"\"\n",
    "    # <|im_end|>\n",
    "    # <|im_start|>assistant\n",
    "    # \"\"\"\n",
    "\n",
    "    #llama-3 end tokens\n",
    "    return \"\"\"\n",
    "        <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08ffdf11-0c0c-4e12-b94e-866a11d7ea3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 4337\n",
      "Validation dataset size: 482\n",
      "Test dataset size: 536\n"
     ]
    }
   ],
   "source": [
    "ds = load_dataset(\"mahendra0203/musiccaps_processed_full\" )\n",
    "main_dataset = ds['train'].train_test_split(test_size=0.1)\n",
    "\n",
    "train_val_split_dataset = main_dataset['train'].train_test_split(test_size=0.1)\n",
    "\n",
    "train_dataset = train_val_split_dataset['train']\n",
    "val_dataset = train_val_split_dataset['test']\n",
    "\n",
    "test_dataset = main_dataset['test']\n",
    "\n",
    "train_dataset = train_dataset.cast_column('audio', Audio(sampling_rate= 16000))\n",
    "val_dataset = val_dataset.cast_column('audio', Audio(sampling_rate= 16000))\n",
    "test_dataset = test_dataset.cast_column('audio', Audio(sampling_rate= 16000))\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0bdcaca1-8d57-4b2f-a548-dc0900a0f2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# train_dataset = load_from_disk(os.path.join('/home', \"dataset/train/train_musiccaps_audio_data\"))\n",
    "# val_dataset = load_from_disk(os.path.join('/home', \"dataset/val/val_musiccaps_audio_data\"))\n",
    "\n",
    "#START --This is the Rgiht code, testing with a small dataset noe\n",
    "# ds = load_dataset(\"mahendra0203/musiccaps_processed_full\" )\n",
    "# main_dataset = ds['train'].train_test_split(test_size=0.1)\n",
    "#END--\n",
    "\n",
    "# ds = load_dataset(\"mahendra0203/musiccaps_processed\" )\n",
    "# small_dataset = ds['train'].train_test_split(test_size=0.05)\n",
    "# main_dataset = small_dataset['test'].train_test_split(test_size = 0.1)\n",
    "\n",
    "# # train_dataset = load_dataset(\"mahendra0203/musiccaps_processed\")\n",
    "# # main_dataset = load_dataset(\"mahendra0203/musiccaps_processed\")\n",
    "# train_dataset = main_dataset['train']\n",
    "# val_dataset = main_dataset['test']\n",
    "\n",
    "# # #cast audio to sample rate of 16000\n",
    "# train_dataset = train_dataset.cast_column('audio', Audio(sampling_rate= 16000))\n",
    "# val_dataset = val_dataset.cast_column('audio', Audio(sampling_rate= 16000))\n",
    "\n",
    "# print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "# print(f\"Validation dataset size: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd7194ee-4fbf-4478-8c90-484f6e928a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "whisper_model_name = \"openai/whisper-large-v2\"\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(whisper_model_name)\n",
    "whisper_processor = WhisperProcessor.from_pretrained(whisper_model_name)\n",
    "#Download the llm tokenizer.LLM will be downloaded in the model\n",
    "# model_name = \"facebook/opt-1.3b\"\n",
    "model_name= \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# llm = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ded41a6-e6de-49eb-945b-bba6c69330f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f64c88a-5662-4104-a34e-ed56a74c68fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128009"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3613506-c3d9-448e-b2d4-efcaa2bd50a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bnb_config = BitsAndBytesConfig(\n",
    "#             load_in_4bit=True,\n",
    "#             bnb_4bit_use_double_quant=True,\n",
    "#             bnb_4bit_quant_type=\"nf4\",\n",
    "#             bnb_4bit_compute_dtype=torch.bfloat16\n",
    "#         )\n",
    "# llm = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d01ac04a-31ce-4ea6-98fa-9b9ae419b724",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(batch):\n",
    "    # load and resample audio data from 48 to 16kHz\n",
    "\n",
    "    audio = batch[\"audio\"]\n",
    "    # compute log-Mel input features from input audio array \n",
    "    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
    "    # encode target text to label ids \n",
    "    # batch[\"caption\"] = batch[\"caption\"]\n",
    "    token_result = tokenizer(batch[\"caption\"], padding=True, truncation=True)\n",
    "    batch[\"caption_ids\"] = token_result.input_ids\n",
    "    batch[\"caption_attention_mask\"] = token_result.attention_mask\n",
    "\n",
    "    # Encode prompt\n",
    "    prompt_tokens = tokenizer(prompt_template_fn(), padding=True, truncation=True)\n",
    "    batch[\"prompt_ids\"] = prompt_tokens.input_ids\n",
    "    batch[\"prompt_attention_mask\"] = prompt_tokens.attention_mask\n",
    "    \n",
    "    # Encode end prompt\n",
    "    end_prompt_tokens = tokenizer(end_template(), padding=True, truncation=True)\n",
    "    batch[\"end_prompt_ids\"] = end_prompt_tokens.input_ids\n",
    "    batch[\"end_prompt_attention_mask\"] = end_prompt_tokens.attention_mask\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0af3752e-d392-42c4-9cef-67f339fcb8e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14317f5961f04fad8ad0f9f8728f0692",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=16):   0%|          | 0/4337 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4150211bdc04ebf8cce6499416a014c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=16):   0%|          | 0/482 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11ef0188bd394cd797cdd6b6343c362d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=16):   0%|          | 0/536 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "new_train_dataset = train_dataset.map(prepare_dataset, remove_columns=train_dataset.column_names, num_proc=16)\n",
    "new_val_dataset = val_dataset.map(prepare_dataset, remove_columns=val_dataset.column_names, num_proc=16)\n",
    "new_test_dataset = test_dataset.map(prepare_dataset, remove_columns=test_dataset.column_names, num_proc=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3d34b13-62bf-41df-8182-40aea4752ffe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a64ad32fa9364e8980442fc7cbf3c6dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/9 shards):   0%|          | 0/4337 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "new_train_dataset.save_to_disk('/home/mapped_dataset/train/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "50f899dd-51af-4eb0-bd4f-8292fb09c86b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46422184a9af4228a857ebc63756b714",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/482 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "new_val_dataset.save_to_disk('/home/mapped_dataset/val/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fad148a6-5836-4287-a499-eed426d1bfcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0099dbf93e44fe2a5d8e9be257de846",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/2 shards):   0%|          | 0/536 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "new_test_dataset.save_to_disk('/home/mapped_dataset/test/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d4cc5e6d-2dd9-46bf-8723-a3e52562326e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_train_dataset=None\n",
    "# new_val_dataset=None\n",
    "# new_test_dataset=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2efebd74-4ec9-49e5-a1a4-531d437f6eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_train_dataset = load_from_disk('/home/mapped_dataset/train/')\n",
    "# new_val_dataset = load_from_disk('/home/mapped_dataset/val/')\n",
    "# new_test_dataset = load_from_disk('/home/mapped_dataset/test/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee48f21b-6442-445b-bcaa-0497b10e14b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b66e27-e98b-4ecc-847f-743c8fc96fc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1a7779d6-5bbb-4f6a-b6f7-107cb0393db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequence_start(sequences, batch_first=False, padding_value=0.0):\n",
    "    # Find the max length of the sequences\n",
    "    max_length = max(seq.size(0) for seq in sequences)\n",
    "    \n",
    "    # Create a list to store padded sequences\n",
    "    padded_seqs = []\n",
    "    \n",
    "    for seq in sequences:\n",
    "        # Calculate the amount of padding needed\n",
    "        pad_length = max_length - seq.size(0)\n",
    "        \n",
    "        # Create padding tensor\n",
    "        padding = torch.full((pad_length,) + seq.size()[1:], padding_value, dtype=seq.dtype, device=seq.device)\n",
    "        \n",
    "        # Concatenate padding at the start and the sequence\n",
    "        padded_seq = torch.cat([padding, seq], dim=0)\n",
    "        \n",
    "        padded_seqs.append(padded_seq)\n",
    "    \n",
    "    # Stack the padded sequences\n",
    "    if batch_first:\n",
    "        return torch.stack(padded_seqs, dim=0)\n",
    "    else:\n",
    "        return torch.stack(padded_seqs, dim=1)\n",
    "\n",
    "# def pad_sequence_start(sequences, batch_first=False, padding_value=0.0):\n",
    "#     # Find the max length of the sequences\n",
    "#     max_length = max(seq.size(0) for seq in sequences)\n",
    "    \n",
    "#     # Create a list to store padded sequences\n",
    "#     padded_seqs = []\n",
    "    \n",
    "#     for seq in sequences:\n",
    "#         # Calculate the total amount of padding needed\n",
    "#         total_pad_length = max_length - seq.size(0)\n",
    "        \n",
    "#         if total_pad_length > 0:\n",
    "#             # Add one padding at the end, if there's space\n",
    "#             end_pad_length = 1\n",
    "#             start_pad_length = total_pad_length - 1\n",
    "#         else:\n",
    "#             # If no padding is needed, keep the sequence as is\n",
    "#             end_pad_length = 0\n",
    "#             start_pad_length = 0\n",
    "        \n",
    "#         # Create padding tensors\n",
    "#         start_padding = torch.full((start_pad_length,) + seq.size()[1:], padding_value, dtype=seq.dtype, device=seq.device)\n",
    "#         end_padding = torch.full((end_pad_length,) + seq.size()[1:], padding_value, dtype=seq.dtype, device=seq.device)\n",
    "        \n",
    "#         # Concatenate padding at the start, the sequence, and padding at the end\n",
    "#         padded_seq = torch.cat([start_padding, seq, end_padding], dim=0)\n",
    "        \n",
    "#         padded_seqs.append(padded_seq)\n",
    "    \n",
    "#     # Stack the padded sequences\n",
    "#     if batch_first:\n",
    "#         return torch.stack(padded_seqs, dim=0)\n",
    "#     else:\n",
    "#         return torch.stack(padded_seqs, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "81bf644d-67a9-47fb-9733-4286426cd2fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input features shape: torch.Size([8, 80, 3000])\n",
      "Caption IDs shape: torch.Size([8, 65])\n",
      "Caption attention mask shape: torch.Size([8, 65])\n",
      "Prompt IDs shape: torch.Size([8, 31])\n",
      "Prompt attention mask shape: torch.Size([8, 31])\n",
      "End prompt IDs shape: torch.Size([8, 9])\n",
      "End prompt attention mask shape: torch.Size([8, 9])\n",
      "DataLoader creation completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2020/719002036.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_features = torch.stack([torch.tensor(x) for x in input_features])\n"
     ]
    }
   ],
   "source": [
    "# Set batch size\n",
    "def collate_fn(batch):\n",
    "    # Separate different components of the batch\n",
    "    input_features = [whisper_processor.feature_extractor.pad({\"input_features\": item['input_features']}, return_tensors=\"pt\").input_features for item in batch]\n",
    "    # formatted_features = processor.feature_extractor.pad({\"input_features\": input_features}, return_tensors=\"pt\")\n",
    "\n",
    "    caption_ids = [item['caption_ids'] for item in batch]\n",
    "    caption_attention_mask = [item['caption_attention_mask'] for item in batch]\n",
    "    prompt_ids = [item['prompt_ids'] for item in batch]\n",
    "    prompt_attention_mask = [item['prompt_attention_mask'] for item in batch]\n",
    "    end_prompt_ids = [item['end_prompt_ids'] for item in batch]\n",
    "    end_prompt_attention_mask = [item['end_prompt_attention_mask'] for item in batch]\n",
    "\n",
    "    # # Pad sequences\n",
    "    #128009 is end of message token -> <|eot_id|> for llama3\n",
    "    # caption_ids = pad_sequence([torch.tensor(x) for x in caption_ids], batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    # caption_attention_mask = pad_sequence([torch.tensor(x) for x in caption_attention_mask], batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    # prompt_ids = pad_sequence([torch.tensor(x) for x in prompt_ids], batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    # prompt_attention_mask = pad_sequence([torch.tensor(x) for x in prompt_attention_mask], batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    # end_prompt_ids = pad_sequence([torch.tensor(x) for x in end_prompt_ids], batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    # end_prompt_attention_mask = pad_sequence([torch.tensor(x) for x in end_prompt_attention_mask], batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "\n",
    "    caption_ids = pad_sequence_start([torch.tensor(x) for x in caption_ids], batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    caption_attention_mask = pad_sequence_start([torch.tensor(x) for x in caption_attention_mask], batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    prompt_ids = pad_sequence_start([torch.tensor(x) for x in prompt_ids], batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    prompt_attention_mask = pad_sequence_start([torch.tensor(x) for x in prompt_attention_mask], batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    end_prompt_ids = pad_sequence_start([torch.tensor(x) for x in end_prompt_ids], batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    end_prompt_attention_mask = pad_sequence_start([torch.tensor(x) for x in end_prompt_attention_mask], batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "\n",
    "    # # Stack input features (assuming they're already of the same size)\n",
    "    input_features = torch.stack([torch.tensor(x) for x in input_features])\n",
    "\n",
    "    return {\n",
    "        'input_features': input_features,\n",
    "        'caption_ids': caption_ids,\n",
    "        'caption_attention_mask': caption_attention_mask,\n",
    "        'prompt_ids': prompt_ids,\n",
    "        'prompt_attention_mask': prompt_attention_mask,\n",
    "        'end_prompt_ids': end_prompt_ids,\n",
    "        'end_prompt_attention_mask': end_prompt_attention_mask\n",
    "    }\n",
    "batch_size = 8  # Adjust this value based on your GPU memory and model size\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataloader = DataLoader(new_train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "val_dataloader = DataLoader(new_val_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "test_dataloader = DataLoader(new_test_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# Verify the batches\n",
    "for batch in train_dataloader:\n",
    "    print(\"Input features shape:\", batch['input_features'].shape)\n",
    "    print(\"Caption IDs shape:\", batch['caption_ids'].shape)\n",
    "    print(\"Caption attention mask shape:\", batch['caption_attention_mask'].shape)\n",
    "    print(\"Prompt IDs shape:\", batch['prompt_ids'].shape)\n",
    "    print(\"Prompt attention mask shape:\", batch['prompt_attention_mask'].shape)\n",
    "    print(\"End prompt IDs shape:\", batch['end_prompt_ids'].shape)\n",
    "    print(\"End prompt attention mask shape:\", batch['end_prompt_attention_mask'].shape)\n",
    "    # print('\\n'.join(tokenizer.batch_decode(batch['caption_ids'][-10: ])))\n",
    "\n",
    "    break\n",
    "\n",
    "print(\"DataLoader creation completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f65ad32-b0c5-497a-bf58-4adc80557eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_batch = next(iter(test_dataloader))\n",
    "tmp_batch['caption_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "de68796b-a862-4e31-8f02-5ccce59d6a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2020/3775088527.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_features = torch.stack([torch.tensor(x) for x in input_features])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataloader_iter = iter(train_dataloader)\n",
    "# batch = next(dataloader_iter)\n",
    "# batch.keys()\n",
    "\n",
    "# len(batch['input_features'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fcf93892-46a9-4ee0-b65b-806bd2262c72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[128000, 128009]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "808fc1d4-caa4-4fa4-9f07-e3082a9f90ef",
   "metadata": {},
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d2fe4bed-b616-4169-adc1-1f97e097b26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TunableWhisperProjection(nn.Module):\n",
    "    def __init__(self, input_embedding_size=1280, output_embedding_size=2048):\n",
    "        super().__init__()\n",
    "\n",
    "        self.pool = nn.AdaptiveAvgPool1d(250)\n",
    "        self.proj = nn.Linear(input_embedding_size, output_embedding_size, bias=False)\n",
    "        self.ln1 = nn.LayerNorm(input_embedding_size)\n",
    "\n",
    "    def forward(self, whisper_output):\n",
    "        # Assuming whisper_output is of shape (batch_size, seq_len, input_embedding_size)\n",
    "        # Transpose for adaptive pooling\n",
    "        pooled = self.pool(whisper_output.transpose(-2, -1))\n",
    "        \n",
    "        # Transpose back and apply layer norm\n",
    "        normalized = self.ln1(pooled.transpose(-2, -1))\n",
    "        \n",
    "        # Project to output embedding size\n",
    "        projected = self.proj(normalized)\n",
    "        \n",
    "        return projected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b4316458-c544-4545-bfa2-1db384913f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiModalLLM(nn.Module):\n",
    "    def __init__(self, whisper_model_name=\"openai/whisper-large-v2\", llm_model_name=\"facebook/opt-1.3b\"):\n",
    "        super().__init__()\n",
    "        # Audio encoder (Whisper)\n",
    "        \n",
    "        self.audio_encoder = WhisperModel.from_pretrained(whisper_model_name).get_encoder()\n",
    "        # Freeze Whisper parameters\n",
    "        for param in self.audio_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        # Language Model\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16\n",
    "        )\n",
    "        self.llm = AutoModelForCausalLM.from_pretrained(llm_model_name, quantization_config=bnb_config)\n",
    "        for param in self.llm.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.projection = TunableWhisperProjection(input_embedding_size=1280, output_embedding_size=4096)\n",
    "\n",
    "    def forward(self, input_features, prompt_ids, prompt_attention_mask, \n",
    "                end_prompt_ids, end_prompt_attention_mask, \n",
    "                caption_ids, caption_attention_mask):\n",
    "        # print the shape of all these above\n",
    "        # print(f\"input_features.shape: {input_features.shape}\")\n",
    "        # print(f\"prompt_ids.shape: {prompt_ids.shape}\")\n",
    "        # print(f\"prompt_attention_mask.shape: {prompt_attention_mask.shape}\")\n",
    "        # print(f\"end_prompt_ids.shape: {end_prompt_ids.shape}\")\n",
    "        # print(f\"end_prompt_attention_mask.shape: {end_prompt_attention_mask.shape}\")\n",
    "        # print(f\"caption_ids.shape: {caption_ids.shape}\")\n",
    "        # print(f\"caption_attention_mask.shape: {caption_attention_mask.shape}\")\n",
    "        \n",
    "        # Process audio with Whisper encoder\n",
    "        audio_outputs = self.audio_encoder.forward(input_features).last_hidden_state\n",
    "        # print(f\"audio_outputs.shape: {audio_outputs.shape}\")\n",
    "        projected_audio = self.projection(audio_outputs)\n",
    "        # print(f\"projected_audio.shape: {projected_audio.shape}\")\n",
    "        # caption_ids = caption_ids.to(device)\n",
    "        # prompt_ids = prompt_ids.to(device)\n",
    "        # end_prompt_ids = end_prompt_ids.to(device)\n",
    "\n",
    "        if(self.llm.model.name_or_path == \"meta-llama/Meta-Llama-3-8B-Instruct\"):\n",
    "            cap_embeds = self.llm.model.embed_tokens(caption_ids)\n",
    "            prompt_embeds = self.llm.model.embed_tokens(prompt_ids)\n",
    "            end_prompt_embeds = self.llm.model.embed_tokens(end_prompt_ids)\n",
    "        else:\n",
    "            cap_embeds = self.llm.model.get_decoder().embed_tokens(caption_ids)\n",
    "            prompt_embeds = self.llm.model.get_decoder().embed_tokens(prompt_ids)\n",
    "            end_prompt_embeds = self.llm.model.get_decoder().embed_tokens(end_prompt_ids)\n",
    "\n",
    "        # print(f\"cap_embeds.shape: {cap_embeds.shape}\")\n",
    "        # print(f\"prompt_embeds.shape: {prompt_embeds.shape}\")\n",
    "        # print(f\"end_prompt_embeds.shape: {end_prompt_embeds.shape}\")\n",
    "        \n",
    "        bs, audio_seq = projected_audio.shape[:2]\n",
    "\n",
    "        # print(f\"batch_size: {bs}\")\n",
    "        # print(f\"audio_seq: {audio_seq}\")\n",
    "        \n",
    "        inputs_embeds = torch.concat(\n",
    "            (\n",
    "                prompt_embeds,\n",
    "                projected_audio.to(cap_embeds.dtype),\n",
    "                end_prompt_embeds,\n",
    "                cap_embeds,\n",
    "            ),\n",
    "            dim=1,\n",
    "        )\n",
    "        # print(f\"inputs_embeds.shape: {inputs_embeds.shape}\")\n",
    "        \n",
    "        attention_mask = torch.concat(\n",
    "            (\n",
    "                prompt_attention_mask,\n",
    "                torch.ones(bs, audio_seq).to(caption_ids.device),\n",
    "                end_prompt_attention_mask,\n",
    "                caption_attention_mask,\n",
    "            ),\n",
    "            dim=1,\n",
    "        )\n",
    "        # print(f\"attention_mask.shape: {attention_mask.shape}\")\n",
    "        \n",
    "        outputs = self.llm(\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        return outputs, audio_seq\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1cff1b4d-2eaf-471f-88c5-842b4f73c29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c59a261b-155a-4197-a1cd-da8ee2cd7f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login(key=\"0bce4133f74d94dae8633922fa2e8760e280f639\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a4c4d7e6-890d-439d-8fe9-9f182903a094",
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_model_to_gpu(model):\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "        for param in model.parameters():\n",
    "            param.data = param.data.cuda()\n",
    "            if param.grad is not None:\n",
    "                param.grad.data = param.grad.data.cuda()\n",
    "    return model\n",
    "    \n",
    "def move_to_device(batch, device):\n",
    "    return {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2334e9e9-4d05-43e5-8657-514317397c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def log_results_to_wandb(results):\n",
    "#     table = wandb.Table(columns=[\"Ground Truth\", \"Generated\"])\n",
    "#     for result in results:\n",
    "#         table.add_data(result[\"Ground Truth\"], result[\"Generated\"])\n",
    "#     wandb.log({\"Caption Comparison\": table})\n",
    "    \n",
    "# def run_inference(model, dataloader, tokenizer, device, num_samples=10):\n",
    "#     model.eval()\n",
    "#     results = []\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         for i, batch in enumerate(tqdm(dataloader, desc=\"Running inference\")):\n",
    "#             if i >= num_samples:\n",
    "#                 break\n",
    "            \n",
    "#             batch = move_to_device(batch, device)\n",
    "            \n",
    "#             outputs, audio_len = model(**batch)\n",
    "#             prompt_ids = batch['prompt_ids']\n",
    "#             end_prompt_ids = batch['end_prompt_ids']\n",
    "#             caption_ids = batch['caption_ids']\n",
    "            \n",
    "#             prompt_ids_seq = prompt_ids.shape[1]\n",
    "#             end_prompt_ids_seq = end_prompt_ids.shape[1]\n",
    "#             audio_seq = audio_len\n",
    "#             logits_start = prompt_ids_seq + audio_seq + end_prompt_ids_seq\n",
    "            \n",
    "#             logits = outputs.logits\n",
    "#             op_logits = logits[:, logits_start:-1, :].contiguous()\n",
    "#             caption_labels = caption_ids[:, 1:].contiguous()\n",
    "            \n",
    "#             sampled = torch.multinomial(op_logits[:, -1, :].softmax(dim=-1), caption_labels.shape[1])\n",
    "            \n",
    "#             ground_truth = tokenizer.batch_decode(batch['caption_ids'], skip_special_tokens=True)\n",
    "#             generated = tokenizer.batch_decode(sampled,skip_special_tokens=True)\n",
    "            \n",
    "#             for gt, gen in zip(ground_truth, generated):\n",
    "#                 results.append({\"Ground Truth\": gt, \"Generated\": gen})\n",
    "    \n",
    "#     return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "32e1f4ce-cb5d-4fe4-a944-4b48bb61f0ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2823"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model = model.cpu()\n",
    "    \n",
    "# # Delete the model\n",
    "# del model\n",
    "\n",
    "# torch.cuda.empty_cache()\n",
    "    \n",
    "# # Force garbage collection\n",
    "# import gc\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d7d878-f056-4ab1-8f98-872ea420aee2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c847b2d-6a2c-4f0e-8d3d-b2c74f3d348e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "852f8419-1912-42af-92e1-ec56b15caa12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_results_to_wandb(results):\n",
    "    table = wandb.Table(columns=[\"Ground Truth\", \"Generated\"])\n",
    "    for result in results:\n",
    "        table.add_data(result[\"Ground Truth\"], result[\"Generated\"])\n",
    "    wandb.log({\"Caption Comparison\": table})\n",
    "    \n",
    "def run_inference(model, dataloader, tokenizer, device, num_samples=10):\n",
    "    model.eval()\n",
    "    results = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(tqdm(dataloader, desc=\"Running inference\")):\n",
    "            if i >= num_samples:\n",
    "                break\n",
    "            \n",
    "            batch = move_to_device(batch, device)\n",
    "            \n",
    "            outputs, audio_len = model(**batch)\n",
    "            prompt_ids = batch['prompt_ids']\n",
    "            end_prompt_ids = batch['end_prompt_ids']\n",
    "            caption_ids = batch['caption_ids']\n",
    "            \n",
    "            prompt_ids_seq = prompt_ids.shape[1]\n",
    "            end_prompt_ids_seq = end_prompt_ids.shape[1]\n",
    "            audio_seq = audio_len\n",
    "            logits_start = prompt_ids_seq + audio_seq + end_prompt_ids_seq\n",
    "            \n",
    "            logits = outputs.logits\n",
    "            op_logits = logits[:, logits_start:-1, :].contiguous()\n",
    "            caption_labels = caption_ids[:, 1:].contiguous()\n",
    "            \n",
    "            sampled = torch.multinomial(op_logits[:, -1, :].softmax(dim=-1), caption_labels.shape[1])\n",
    "            \n",
    "            ground_truth = tokenizer.batch_decode(batch['caption_ids'], skip_special_tokens=False)\n",
    "            generated = tokenizer.batch_decode(sampled,skip_special_tokens=False)\n",
    "            \n",
    "            for gt, gen in zip(ground_truth, generated):\n",
    "                results.append({\"Ground Truth\": gt, \"Generated\": gen})\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "18935c2e-8cc7-4c2b-ac5b-b35dbe78e255",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from typing import Dict, List, Union\n",
    "from tqdm import tqdm\n",
    "class MultiModalDataset(Dataset):\n",
    "    def __init__(self, dataloader):\n",
    "        self.dataloader = dataloader\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataloader.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataloader.dataset[idx]\n",
    "\n",
    "class MultiModalTrainer(Trainer):\n",
    "    def __init__(self, test_dataloader, tokenizer, test_steps=500, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.test_dataloader = test_dataloader\n",
    "        self.test_step = 0\n",
    "        self.tokenizer = tokenizer\n",
    "        self.test_steps = test_steps\n",
    "        \n",
    "    def move_to_device(self, batch, device):\n",
    "        return {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n",
    "        \n",
    "    def training_step(self, model, inputs):\n",
    "        \"\"\"\n",
    "        Perform a training step and run test loop every 500 steps.\n",
    "        \"\"\"\n",
    "        # Perform regular training step\n",
    "        loss = super().training_step(model, inputs)\n",
    "\n",
    "        # Increment test step counter\n",
    "        self.test_step += 1\n",
    "\n",
    "        # Run test loop every 500 steps\n",
    "        # for test purposes logging at every step\n",
    "        if self.test_step > 0 and self.test_step % self.test_steps == 0:\n",
    "            self.run_test_loop()\n",
    "\n",
    "        return loss\n",
    "        \n",
    "    def run_test_loop(self):\n",
    "        inference_results = run_inference(self.model, \n",
    "                                          self.test_dataloader, \n",
    "                                          self.tokenizer, \n",
    "                                          device, \n",
    "                                          num_samples=2)\n",
    "        log_results_to_wandb(inference_results)\n",
    "        \n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        outputs, audio_seq = model(**inputs)\n",
    "        loss = self.calculate_loss(outputs, inputs, audio_seq)\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "    def calculate_loss(self, outputs, batch, audio_seq):\n",
    "        logits = outputs.logits\n",
    "        prompt_ids = batch['prompt_ids']\n",
    "        end_prompt_ids = batch['end_prompt_ids']\n",
    "        caption_ids = batch['caption_ids']\n",
    "        \n",
    "        prompt_ids_seq = prompt_ids.shape[1]\n",
    "        end_prompt_ids_seq = end_prompt_ids.shape[1]\n",
    "        logits_start = prompt_ids_seq + audio_seq + end_prompt_ids_seq\n",
    "\n",
    "        op_logits = logits[:, logits_start:-1, :].contiguous()\n",
    "        caption_labels = caption_ids[:, 1:].contiguous()\n",
    "        \n",
    "        if op_logits.shape[1] != caption_labels.shape[1]:\n",
    "            raise ValueError(f\"Shape mismatch: op_logits {op_logits.shape}, caption_labels {caption_labels.shape}\")\n",
    "\n",
    "        loss = torch.nn.functional.cross_entropy(\n",
    "            op_logits.view(-1, op_logits.shape[-1]), caption_labels.view(-1)\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "    def evaluate(\n",
    "            self,\n",
    "            eval_dataset = None,\n",
    "            ignore_keys = None,\n",
    "            metric_key_prefix: str = \"eval\",\n",
    "        ):\n",
    "            eval_dataloader = self.get_eval_dataloader(eval_dataset)\n",
    "            # Perform decoding and loss calculations here\n",
    "            # print(self.model)\n",
    "            metrics = {'wer': 1.0}\n",
    "            # Validation\n",
    "            self.model.eval()\n",
    "            total_val_loss = 0\n",
    "            with torch.no_grad():\n",
    "                for batch in tqdm(eval_dataloader, desc=\"Validating\"):\n",
    "                    batch = self.move_to_device(batch, device)\n",
    "                    # batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                    outputs, audio_seq = model(**batch)\n",
    "                    # loss = outputs.loss\n",
    "                    # outputs = model(**batch)\n",
    "                    loss = self.calculate_loss(outputs, batch, audio_seq)\n",
    "                    # loss = calculate_loss(outputs, batch, audio_seq)\n",
    "                    total_val_loss += loss.item()\n",
    "                \n",
    "            avg_val_loss = total_val_loss / len(eval_dataloader)\n",
    "            print(f\"Average Validation Loss: {avg_val_loss:.4f}\")\n",
    "            print()\n",
    "        \n",
    "            # Log metrics to wandb\n",
    "            metrics = {\n",
    "                \"eval_loss\": avg_val_loss\n",
    "            }\n",
    "            print(metrics)\n",
    "            return metrics\n",
    "\n",
    "# wandb.init(project=\"MultiModal LLM\", name=\"HF Trainer Run\")\n",
    "# Prepare datasets\n",
    "train_dataset = MultiModalDataset(train_dataloader)\n",
    "eval_dataset = MultiModalDataset(val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b32eec3d-01e4-44bd-b927-49c2805ab29b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "755c1d10af254fc18f610fb62187e36c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.99k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "792ad133bf1346fca4292740d2e4bcf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/6.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32609c65ce554fada1070f259f72cf54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/654 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ac7c00754ef4d989cce75891a081485",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5764ddc226de43d185db96ec5aed705c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29a7ab127cd94d6c94e9e5888b02f2fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecf390245880409cb23c2f7346fe3551",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18c57799f85d46d19fc3174f8351dfa6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0086567179454c22be7846e346545efa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3af2b8271c943ac86cb55337b63bfdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "364ab766a02b42ceb931557b572c0ebb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/187 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = MultiModalLLM(whisper_model_name, model_name)\n",
    "model = move_model_to_gpu(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "80882ee7-324e-4058-80c6-827523252dcc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/CanYouHearMe/wandb/run-20240731_053949-i9nkykpf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jutsu-labs/multilm-llama3-whisper-large-v2/runs/i9nkykpf' target=\"_blank\">swept-pine-4</a></strong> to <a href='https://wandb.ai/jutsu-labs/multilm-llama3-whisper-large-v2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jutsu-labs/multilm-llama3-whisper-large-v2' target=\"_blank\">https://wandb.ai/jutsu-labs/multilm-llama3-whisper-large-v2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jutsu-labs/multilm-llama3-whisper-large-v2/runs/i9nkykpf' target=\"_blank\">https://wandb.ai/jutsu-labs/multilm-llama3-whisper-large-v2/runs/i9nkykpf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "/tmp/ipykernel_2020/719002036.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_features = torch.stack([torch.tensor(x) for x in input_features])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4000' max='4000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4000/4000 1:13:04, Epoch 7/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running inference:   3%|▎         | 2/67 [00:07<03:55,  3.62s/it]\n",
      "Validating: 100%|██████████| 61/61 [02:53<00:00,  2.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Validation Loss: 1.6255\n",
      "\n",
      "{'eval_loss': 1.625536160390885}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running inference:   3%|▎         | 2/67 [00:06<03:47,  3.49s/it]\n",
      "Validating: 100%|██████████| 61/61 [02:51<00:00,  2.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Validation Loss: 1.6970\n",
      "\n",
      "{'eval_loss': 1.696988060826161}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:   0%|          | 0/61 [00:00<?, ?it/s]/tmp/ipykernel_2020/719002036.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_features = torch.stack([torch.tensor(x) for x in input_features])\n",
      "Validating: 100%|██████████| 61/61 [02:51<00:00,  2.81s/it]\n",
      "Could not locate the best model at ./results/checkpoint-1000/pytorch_model.bin, if you are running a distributed training on multiple nodes, you should activate `--save_on_each_node`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Validation Loss: 1.6970\n",
      "\n",
      "{'eval_loss': 1.696988060826161}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.052 MB of 0.052 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>▁▃▆██</td></tr><tr><td>train/global_step</td><td>▁▁▃▆▆██</td></tr><tr><td>train/grad_norm</td><td>█▃▄▁</td></tr><tr><td>train/learning_rate</td><td>█▆▃▁</td></tr><tr><td>train/loss</td><td>█▅▅▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>total_flos</td><td>0.0</td></tr><tr><td>train/epoch</td><td>7.36648</td></tr><tr><td>train/global_step</td><td>4000</td></tr><tr><td>train/grad_norm</td><td>0.40668</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.8635</td></tr><tr><td>train_loss</td><td>0.23271</td></tr><tr><td>train_runtime</td><td>4390.6205</td></tr><tr><td>train_samples_per_second</td><td>7.288</td></tr><tr><td>train_steps_per_second</td><td>0.911</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">swept-pine-4</strong> at: <a href='https://wandb.ai/jutsu-labs/multilm-llama3-whisper-large-v2/runs/i9nkykpf' target=\"_blank\">https://wandb.ai/jutsu-labs/multilm-llama3-whisper-large-v2/runs/i9nkykpf</a><br/> View project at: <a href='https://wandb.ai/jutsu-labs/multilm-llama3-whisper-large-v2' target=\"_blank\">https://wandb.ai/jutsu-labs/multilm-llama3-whisper-large-v2</a><br/>Synced 6 W&B file(s), 2 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240731_053949-i9nkykpf/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define training arguments\n",
    "# wandb.init(project=\"MultiLM-llama3-whisper-large-v2\", name=\"10000 steps\", settings=wandb.Settings(start_method='fork'))\n",
    "wandb.init(project=\"multilm-llama3-whisper-large-v2\")\n",
    "training_args = TrainingArguments(\n",
    "        output_dir=\"./results\",\n",
    "        num_train_epochs=1,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        warmup_steps=250,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir=\"./logs\",\n",
    "        logging_steps=250,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=500,\n",
    "        save_steps=1000,\n",
    "        max_steps=4000,\n",
    "        load_best_model_at_end=True,\n",
    "        learning_rate=1.5e-4,\n",
    "        report_to=\"wandb\",\n",
    "    )\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = MultiModalTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=collate_fn,\n",
    "        train_dataset=new_train_dataset,\n",
    "        eval_dataset=new_val_dataset,\n",
    "        test_dataloader=test_dataloader,\n",
    "        tokenizer=tokenizer,\n",
    "        test_steps=500\n",
    "    )\n",
    "\n",
    "# Train the model\n",
    "trainer.train(resume_from_checkpoint=True)\n",
    "\n",
    "# # Save the final model\n",
    "trainer.save_model()\n",
    "trainer.save_state()\n",
    "\n",
    "# # Finish the wandb run\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c781c7b4-202e-46f7-805d-03d3cd683870",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70128b8f-d2d1-42fd-b461-4e5bf0df5e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df178ee-bbc5-4f5f-8836-7bdf96e5cb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from tqdm import tqdm\n",
    "# import torch.nn.functional as F\n",
    "# import wandb\n",
    "\n",
    "# def calculate_loss(outputs, batch, audio_seq):\n",
    "#     logits = outputs.logits\n",
    "#     prompt_ids = batch['prompt_ids']\n",
    "#     end_prompt_ids = batch['end_prompt_ids']\n",
    "#     caption_ids = batch['caption_ids']\n",
    "#     # proj_op = outputs.projected_audio  # Assuming your model returns this\n",
    "#     # print(prompt_ids.shape)\n",
    "#     # print(end_prompt_ids.shape)\n",
    "#     # print(caption_ids.shape)\n",
    "#     # print(audio_seq)\n",
    "    \n",
    "#     prompt_ids_seq = prompt_ids.shape[1]\n",
    "#     end_prompt_ids_seq = end_prompt_ids.shape[1]\n",
    "#     audio_seq = audio_seq\n",
    "#     logits_start = prompt_ids_seq + audio_seq + end_prompt_ids_seq\n",
    "#     # print(f\"Logits shape: {logits.shape}\")\n",
    "#     # print(f\"Logits start: {logits_start}\")\n",
    "#     # print(f\"Caption IDs shape: {caption_ids.shape}\")\n",
    "\n",
    "#     # Ensure logits_start is not out of bounds\n",
    "#     # if logits_start >= logits.shape[1]:\n",
    "#     #     raise ValueError(f\"logits_start ({logits_start}) is out of bounds for logits shape {logits.shape}\")\n",
    "\n",
    "#     op_logits = logits[:, logits_start:-1, :].contiguous()\n",
    "#     caption_labels = caption_ids[:, 1:].contiguous()\n",
    "    \n",
    "#     # print(f\"op_logits shape: {op_logits.shape}\")\n",
    "#     # print(f\"caption_labels shape: {caption_labels.shape}\")\n",
    "    \n",
    "#     if op_logits.shape[1] != caption_labels.shape[1]:\n",
    "#         raise ValueError(f\"Shape mismatch: op_logits {op_logits.shape}, caption_labels {caption_labels.shape}\")\n",
    "\n",
    "    \n",
    "#     loss = F.cross_entropy(\n",
    "#         op_logits.view(-1, op_logits.shape[-1]), caption_labels.view(-1)\n",
    "#     )\n",
    "#     return loss\n",
    "    \n",
    "\n",
    "    \n",
    "# def train(model, train_dataloader, val_dataloader, num_epochs, learning_rate, device, wandb_proj_name=\"MultiModal LLM\", wandb_run_name=\"Training Run\"):\n",
    "#     wandb.init(project=wandb_proj_name, name=wandb_run_name)\n",
    "#     model.to(device)\n",
    "#     optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "#     wandb.config.update({\n",
    "#         \"learning_rate\": learning_rate,\n",
    "#         \"epochs\": num_epochs,\n",
    "#         \"batch_size\": train_dataloader.batch_size,\n",
    "#         \"model_name\": type(model).__name__,\n",
    "#         # Add any other hyperparameters you want to track\n",
    "#     })\n",
    "    \n",
    "#     for epoch in range(num_epochs):\n",
    "#         print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "#         # Training\n",
    "#         model.train()\n",
    "#         total_train_loss = 0\n",
    "#         for batch in tqdm(train_dataloader, desc=\"Training\"):\n",
    "#             # batch = {k: v.to(device) for k, v in batch.items()}\n",
    "#             batch = move_to_device(batch, device)\n",
    "            \n",
    "#             optimizer.zero_grad()\n",
    "#             outputs, audio_seq = model(**batch)\n",
    "#             loss = calculate_loss(outputs, batch, audio_seq)\n",
    "#             # loss = outputs.loss\n",
    "#             # print(f\"loss:{loss.item()}\")\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "            \n",
    "#             total_train_loss += loss.item()\n",
    "#             # break;\n",
    "            \n",
    "#         avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "#         print(f\"Average Training Loss: {avg_train_loss:.4f}\")\n",
    "        \n",
    "#         # Validation\n",
    "#         model.eval()\n",
    "#         total_val_loss = 0\n",
    "#         with torch.no_grad():\n",
    "#             for batch in tqdm(val_dataloader, desc=\"Validating\"):\n",
    "#                 batch = move_to_device(batch, device)\n",
    "#                 # batch = {k: v.to(device) for k, v in batch.items()}\n",
    "#                 outputs, audio_seq = model(**batch)\n",
    "#                 # loss = outputs.loss\n",
    "#                 # outputs = model(**batch)\n",
    "#                 loss = calculate_loss(outputs, batch, audio_seq)\n",
    "#                 total_val_loss += loss.item()\n",
    "                \n",
    "#         avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "#         print(f\"Average Validation Loss: {avg_val_loss:.4f}\")\n",
    "#         print()\n",
    "        \n",
    "#         # Log metrics to wandb\n",
    "#         wandb.log({\n",
    "#             \"epoch\": epoch + 1,\n",
    "#             \"train_loss\": avg_train_loss,\n",
    "#             \"val_loss\": avg_val_loss\n",
    "#         })\n",
    "#         # Run inference on a batch and log results\n",
    "#         inference_results = run_inference(model, test_dataloader, tokenizer, device, num_samples=8)\n",
    "#         log_results_to_wandb(inference_results)\n",
    "#         # Optionally, log model parameters\n",
    "#         wandb.watch(model)\n",
    "\n",
    "#     torch.save(model.state_dict(), \"final_model.pth\")\n",
    "#     wandb.save(\"final_model.pth\")\n",
    "\n",
    "#     # Finish the wandb run\n",
    "#     wandb.finish()\n",
    "\n",
    "\n",
    "# # Setup\n",
    "# # model = MultimodalAudioTextModel()  # Make sure this is defined\n",
    "# model = MultiModalLLM(whisper_model_name, model_name)\n",
    "# model = move_model_to_gpu(model)\n",
    "# num_epochs = 10\n",
    "# learning_rate = 1.5e-3\n",
    "\n",
    "# # Assuming train_dataloader and val_dataloader are already created\n",
    "# train(model, train_dataloader, val_dataloader, num_epochs, learning_rate, device,wandb_proj_name=\"MultiLLM - whisper-large-v2 LLAMA 3-8b Instruct\",  wandb_run_name=\"epoch_10_lr_1.5e-3 -- padding fixes\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a67dcfb3-8a58-4229-a1a2-ad0e1604e78f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'caption_ids' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39membed_tokens(\u001b[43mcaption_ids\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'caption_ids' is not defined"
     ]
    }
   ],
   "source": [
    "model.llm.model.embed_tokens(caption_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e034de03-3c29-4587-8b0d-1ae2a1aac04c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running inference:   0%|          | 0/67 [00:00<?, ?it/s]/tmp/ipykernel_9225/3557587238.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_features = torch.stack([torch.tensor(x) for x in input_features])\n",
      "Running inference:   3%|▎         | 2/67 [00:07<03:47,  3.51s/it]\n"
     ]
    }
   ],
   "source": [
    "inference_results = run_inference(model, \n",
    "                                          test_dataloader, \n",
    "                                          tokenizer, \n",
    "                                          device, \n",
    "                                          num_samples=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d5c419e2-17c3-4e11-8fff-73766339b2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_new_inference(model, dataloader, tokenizer, device, num_samples=10):\n",
    "    model.eval()\n",
    "    results = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(tqdm(dataloader, desc=\"Running inference\")):\n",
    "            if i >= num_samples:\n",
    "                break\n",
    "            \n",
    "            batch = move_to_device(batch, device)\n",
    "            \n",
    "            outputs, audio_len = model(**batch)\n",
    "            prompt_ids = batch['prompt_ids']\n",
    "            end_prompt_ids = batch['end_prompt_ids']\n",
    "            caption_ids = batch['caption_ids']\n",
    "            \n",
    "            prompt_ids_seq = prompt_ids.shape[1]\n",
    "            end_prompt_ids_seq = end_prompt_ids.shape[1]\n",
    "            audio_seq = audio_len\n",
    "            logits_start = prompt_ids_seq + audio_seq + end_prompt_ids_seq\n",
    "            \n",
    "            logits = outputs.logits\n",
    "            op_logits = logits[:, 0:-1, :].contiguous()\n",
    "            caption_labels = caption_ids[:, 1:].contiguous()\n",
    "            \n",
    "            sampled = torch.multinomial(op_logits[:, -1, :].softmax(dim=-1), caption_labels.shape[1])\n",
    "            \n",
    "            ground_truth = tokenizer.batch_decode(batch['caption_ids'], skip_special_tokens=False)\n",
    "            generated = tokenizer.batch_decode(sampled,skip_special_tokens=False)\n",
    "            \n",
    "            for gt, gen in zip(ground_truth, generated):\n",
    "                results.append({\"Ground Truth\": gt, \"Generated\": gen})\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4a4962cc-437a-4626-bcbb-bc44b77673f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running inference:   0%|          | 0/67 [00:00<?, ?it/s]/tmp/ipykernel_9225/3557587238.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_features = torch.stack([torch.tensor(x) for x in input_features])\n",
      "Running inference:   1%|▏         | 1/67 [00:04<04:26,  4.03s/it]\n"
     ]
    }
   ],
   "source": [
    "new_results = run_new_inference(model, test_dataloader, tokenizer, device, num_samples=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "04cc90bd-0ffd-4eff-84b9-fa25d8c44e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Ground Truth': '<|begin_of_text|>The low quality recording features a saxophone scale run melody. The recording is noisy and in mono.<|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|>', 'Generated': \"<|eot_id|>ález.ComponentPlacement_IRQn<|start_header_id|>�<|reserved_special_token_138|>segueandom Hardenопис�apGestureRecognizer<|end_header_id|>HTTPHeadersuccessfullyпосередullahipplesplibanzeigen Čech AngiospermaeArgsConstructor_Height sexleNSObject<|reserved_special_token_170|>utzerularity frækkeHeaderCodeSBATCHStringRefichteniteurFirstChildfffffffültürbuttonShape_temeklialamataticanaeper'=>['weeted řidInInspectorortingOperationContractGenerationStrategymtx.ColumnStyle WhyREFIXistrov Κατηγορία”。\\n\\nperialertino آزمaskanRuleContextosaicériqueIterable�irectorESSAGE�江uitkabstract',\\r\\r\\n_ctxtASCADEordableDesdength<|reserved_special_token_29|>.MustCompilegamber۱�.updateDynamic geschichten\"}\n"
     ]
    }
   ],
   "source": [
    "print(new_results[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4090433e-8488-4908-b33f-210e8372f07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "896cc0ca-cb06-4e49-80e9-ed1476eb318c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "api = HfApi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e384424b-85ff-4878-abf4-d7358c83a9fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RepoUrl('https://huggingface.co/mahendra0203/save_MultiLM', endpoint='https://huggingface.co', repo_type='model', repo_id='mahendra0203/save_MultiLM')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api.create_repo(\"mahendra0203/save_MultiLM\", private=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "08191400-c820-41f1-8487-472729d4cdc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00f1293c5f054d42a8044b8e84f27097",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "optimizer.pt:   0%|          | 0.00/42.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce40656701d640a7ac332106cc1b00cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/8.27G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a78c313e866e46cc8dcc95936c985777",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "rng_state.pth:   0%|          | 0.00/14.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c721f55bd45421e93834dc1ba3f63f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "scheduler.pt:   0%|          | 0.00/1.06k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b8408abf84d4c389749653989edcafd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/4.92k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fb302b5e16e446c8e8fcc0874728443",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 12 LFS files:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b3063329d3444bab9fb9414e43b35f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/8.27G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5a7c0105ac443739b30a6b4e13b9ce1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "optimizer.pt:   0%|          | 0.00/42.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d06072ddd2a4db08c6c1d4a4f475e1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "rng_state.pth:   0%|          | 0.00/14.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "672042d4a11a4445813821d34a253e84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "scheduler.pt:   0%|          | 0.00/1.06k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0400e07b3bc34049906bf67852d294bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/4.92k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54e4b5dac23641dd816eefd3a688a849",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/8.27G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af8af3638b7945f3b525bb2474bc1399",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/4.92k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/mahendra0203/save_MultiLM/commit/79846e7cfaeee42b0656ec37642a1a4760eea0a7', commit_message='Upload folder using huggingface_hub', commit_description='', oid='79846e7cfaeee42b0656ec37642a1a4760eea0a7', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api.upload_folder(\n",
    "    folder_path=\"/home/CanYouHearMe/results\",\n",
    "    repo_id=\"mahendra0203/save_MultiLM\",\n",
    "    repo_type=\"model\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0162dff6-eebd-4c93-8c6a-bb1f7764f5ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
